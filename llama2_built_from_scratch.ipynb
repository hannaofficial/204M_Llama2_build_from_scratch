{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTN07gbsfUyxoo8tTmw2fo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1753bc7d61740069146779ae2635b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d5849ebe8a241b4a1d3783a650fb9fb",
              "IPY_MODEL_03fc89f217ed492887ff2bbe929dc583",
              "IPY_MODEL_5e9e4d1843574218be5e2242aca70ae0"
            ],
            "layout": "IPY_MODEL_e70700e607ac44ddb47f74384669fff4"
          }
        },
        "4d5849ebe8a241b4a1d3783a650fb9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549e31ecdce14daeba8cebbcedcb12e9",
            "placeholder": "​",
            "style": "IPY_MODEL_cfc0f601a17d40b3a4eae1266b57859f",
            "value": "Downloading data: 100%"
          }
        },
        "03fc89f217ed492887ff2bbe929dc583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e98b8d70ceba44a2abd22048352bd437",
            "max": 47496131,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_385c4d0f705943759fbbc26dc0ec2f80",
            "value": 47496131
          }
        },
        "5e9e4d1843574218be5e2242aca70ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c5d195f8cdc4e4d87bb86f9f34abd04",
            "placeholder": "​",
            "style": "IPY_MODEL_a4951a39144d4d619d45af8c07df18fa",
            "value": " 47.5M/47.5M [00:00&lt;00:00, 155MB/s]"
          }
        },
        "e70700e607ac44ddb47f74384669fff4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549e31ecdce14daeba8cebbcedcb12e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfc0f601a17d40b3a4eae1266b57859f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e98b8d70ceba44a2abd22048352bd437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385c4d0f705943759fbbc26dc0ec2f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c5d195f8cdc4e4d87bb86f9f34abd04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4951a39144d4d619d45af8c07df18fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1342ae08ef744e1abf14406fe10fe1e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fb2b4308ee5488ebdf904cda012aa10",
              "IPY_MODEL_0281101eb1d54df4b776266343a625be",
              "IPY_MODEL_da6329d90e27475fa29c5a00158dddca"
            ],
            "layout": "IPY_MODEL_0985979c2a1742c7bd70ff99e00e70e1"
          }
        },
        "0fb2b4308ee5488ebdf904cda012aa10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7cfd6b5b654aa6a1e7f63e4989337d",
            "placeholder": "​",
            "style": "IPY_MODEL_e3fea70996284e65b6ce94216013360d",
            "value": "Downloading data: 100%"
          }
        },
        "0281101eb1d54df4b776266343a625be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7232284f4d2049e0a2c2052bcbfece3d",
            "max": 11752147,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9098cd144f748ee860d7161be37cf4c",
            "value": 11752147
          }
        },
        "da6329d90e27475fa29c5a00158dddca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3805767a9cde4a0787c8ac5f5b201a93",
            "placeholder": "​",
            "style": "IPY_MODEL_c49a4a47e4084546a6d669e9c1390255",
            "value": " 11.8M/11.8M [00:00&lt;00:00, 246MB/s]"
          }
        },
        "0985979c2a1742c7bd70ff99e00e70e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7cfd6b5b654aa6a1e7f63e4989337d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3fea70996284e65b6ce94216013360d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7232284f4d2049e0a2c2052bcbfece3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9098cd144f748ee860d7161be37cf4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3805767a9cde4a0787c8ac5f5b201a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c49a4a47e4084546a6d669e9c1390255": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3dc74e64214845a39eae0a0620593235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38500e391434470fbf10da7f24643904",
              "IPY_MODEL_0fd0b93dbb444964855def2efbbf62ef",
              "IPY_MODEL_8b8f5369d28f403889e0534b61b8dd35"
            ],
            "layout": "IPY_MODEL_e6992589b6984f1b9689db532ffb6b9f"
          }
        },
        "38500e391434470fbf10da7f24643904": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b89b9bff5f8d44cb9e58790ba2c509dd",
            "placeholder": "​",
            "style": "IPY_MODEL_c7de23614b4144308a6cd8c472a5676f",
            "value": "Downloading data: 100%"
          }
        },
        "0fd0b93dbb444964855def2efbbf62ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e001a40c3264f668eb7205beae3b125",
            "max": 12246618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcf017cf22d749c1b2f78c527317c046",
            "value": 12246618
          }
        },
        "8b8f5369d28f403889e0534b61b8dd35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2016f006a79c4433986164efb4e97df1",
            "placeholder": "​",
            "style": "IPY_MODEL_da81c658c3ad4183a7075509ec968a71",
            "value": " 12.2M/12.2M [00:00&lt;00:00, 220MB/s]"
          }
        },
        "e6992589b6984f1b9689db532ffb6b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b89b9bff5f8d44cb9e58790ba2c509dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7de23614b4144308a6cd8c472a5676f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e001a40c3264f668eb7205beae3b125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcf017cf22d749c1b2f78c527317c046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2016f006a79c4433986164efb4e97df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da81c658c3ad4183a7075509ec968a71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57d2b5004caa4289b9a028c609a48f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8778048e3d6b4cb69c58ec308fcb83c6",
              "IPY_MODEL_86f5ea248ea04272b94d1832075d66b6",
              "IPY_MODEL_b0e6e37dcd4740fcb1a501c4ccbd82f2"
            ],
            "layout": "IPY_MODEL_8737489066f04c3db5a5509dcca7a412"
          }
        },
        "8778048e3d6b4cb69c58ec308fcb83c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6990515bef6e4f35a9b365625c177ffb",
            "placeholder": "​",
            "style": "IPY_MODEL_f0ac60ce7c59471ab7bfd7a22bc753b3",
            "value": "Generating train split: 100%"
          }
        },
        "86f5ea248ea04272b94d1832075d66b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c59239ccc0c1429eb832e69d664ab5ff",
            "max": 39905,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68bad90bf8d9460182a5f73e3dcfa571",
            "value": 39905
          }
        },
        "b0e6e37dcd4740fcb1a501c4ccbd82f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea7914aa3c7d41dd90f0d5e438bf9d2a",
            "placeholder": "​",
            "style": "IPY_MODEL_98ef4cc9e1b7426591b73c928b3e46b0",
            "value": " 39905/39905 [00:02&lt;00:00, 16955.41 examples/s]"
          }
        },
        "8737489066f04c3db5a5509dcca7a412": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6990515bef6e4f35a9b365625c177ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0ac60ce7c59471ab7bfd7a22bc753b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c59239ccc0c1429eb832e69d664ab5ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68bad90bf8d9460182a5f73e3dcfa571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea7914aa3c7d41dd90f0d5e438bf9d2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ef4cc9e1b7426591b73c928b3e46b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3305e95044034f6ea903fca2956788a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa78236aed7c4f58ab87ba69c0c56633",
              "IPY_MODEL_1907649aa49f405c80d40492b123b1af",
              "IPY_MODEL_a1c88925b0c84b9899724088c761c3b3"
            ],
            "layout": "IPY_MODEL_b24c6e82f58f4418ad7681a1752cbc00"
          }
        },
        "fa78236aed7c4f58ab87ba69c0c56633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6468c9f46ed4458bbafa298079cf985b",
            "placeholder": "​",
            "style": "IPY_MODEL_e50364e0dd0c47c5aa9b2b3ced0e14e7",
            "value": "Generating test split: 100%"
          }
        },
        "1907649aa49f405c80d40492b123b1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c1a8beef8245b897a36a5454b5e7c6",
            "max": 10003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27146f936e4747ceb55f8722aa174985",
            "value": 10003
          }
        },
        "a1c88925b0c84b9899724088c761c3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eceb511289584fb485ee37310a8800e8",
            "placeholder": "​",
            "style": "IPY_MODEL_efd9471a5034441b8f514b7a37849429",
            "value": " 10003/10003 [00:00&lt;00:00, 15899.65 examples/s]"
          }
        },
        "b24c6e82f58f4418ad7681a1752cbc00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6468c9f46ed4458bbafa298079cf985b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50364e0dd0c47c5aa9b2b3ced0e14e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2c1a8beef8245b897a36a5454b5e7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27146f936e4747ceb55f8722aa174985": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eceb511289584fb485ee37310a8800e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd9471a5034441b8f514b7a37849429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfab93490b1144328eb3705d3c86418f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46f5c65ef8004053b6c84963ef51c416",
              "IPY_MODEL_cc78152ef350447485b4e4b4c79ed8f0",
              "IPY_MODEL_c5975c9a807643fdb5c3fd3d3e93b6f5"
            ],
            "layout": "IPY_MODEL_4fdec6411a2e41bba366a4e83f8f9e1a"
          }
        },
        "46f5c65ef8004053b6c84963ef51c416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccb7e6d540f74eb6a3275bf082daad41",
            "placeholder": "​",
            "style": "IPY_MODEL_3bcb91091c6e4358b33e8ae17e3857d9",
            "value": "Generating validation split: 100%"
          }
        },
        "cc78152ef350447485b4e4b4c79ed8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e990e9f45e28455884f7e3b11e83f02e",
            "max": 10042,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d77c496f2e0a41738ac7f93323ab5a34",
            "value": 10042
          }
        },
        "c5975c9a807643fdb5c3fd3d3e93b6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03744b4189be4a26bdf70ee41ce27591",
            "placeholder": "​",
            "style": "IPY_MODEL_3b2c962470c64b6dbbc6993f6dd9cb8d",
            "value": " 10042/10042 [00:00&lt;00:00, 18466.86 examples/s]"
          }
        },
        "4fdec6411a2e41bba366a4e83f8f9e1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccb7e6d540f74eb6a3275bf082daad41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bcb91091c6e4358b33e8ae17e3857d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e990e9f45e28455884f7e3b11e83f02e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d77c496f2e0a41738ac7f93323ab5a34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "03744b4189be4a26bdf70ee41ce27591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b2c962470c64b6dbbc6993f6dd9cb8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannaofficial/204M_Llama2_build_from_scratch/blob/main/llama2_built_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did I write this code\n",
        "\n",
        "I take GPT3 code as reference that I learn from adrej karpathy and then I modified the code based on llama research paper llama 1,2,3\n",
        "\n",
        "Only difference bet llama 2 and 3 is scaling(50x more parameter than llama2) but all the fundamental unit are moreover same that's why I write llama2 not 3.\n",
        "\n",
        "Even my model is tiny."
      ],
      "metadata": {
        "id": "G2LA-RbEENN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the diiference in LLama wrt gpt3\n",
        "\n",
        "* RMS normalization compare to layernorm\n",
        "* Rope with respected to static position embedding with training parameter\n",
        "* Swiglu wrt Gelu\n",
        "* implementing xformers instead of Flash attention it is same but more efficient then direct implementation and also use the latest top attention\n",
        "* custom linear implemention and save checkpoint for ram efficiency"
      ],
      "metadata": {
        "id": "biIj49jzRgza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton    #xformers is install due to implement efficient flash attention develop by meta\n",
        "!pip install xformers\n",
        "!pip install tiktoken\n",
        "!pip install xformers --upgrade"
      ],
      "metadata": {
        "id": "GDup2riwLNR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYPvoemh32xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498c2bf6-4859-4c2f-8d01-9a1b44aa46b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda is available\n",
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import inspect\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "print(f'{device} is available')\n",
        "\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "#set up DDP (distributed data parrallel)\n",
        "#trochrun command set the env varable RANK, LOCALRANK and WORLD SIZE\n",
        "ddp = int(os.environ.get('RANK',-1)) != -1\n",
        "if ddp:\n",
        "  assert torch.cuda.is_available(), 'for now i think we need cuda'\n",
        "  init_process_group(backend='nccl')\n",
        "  ddp_rank = int(os.environ['RANK'])  #identification of each gpu\n",
        "  ddp_local_rank = int(os.environ['LOCAL_RANK'])  #rank of the gpu on single node\n",
        "  ddp_world_size = int(os.environ['WORLD_SIZE'])  #total num of processes/gpu running\n",
        "  device = f'cuda:{ddp_local_rank}'\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank == 0\n",
        "else:\n",
        "  #vanilla, non-DDP run\n",
        "  ddp_rank = 0\n",
        "  ddp_local_rank = 0\n",
        "  ddp_world_size = 1\n",
        "  master_process = True\n",
        "  #attempt to autodetect device\n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "  print(f\"using device: {device}\")\n",
        "\n",
        "#------------------------------\n",
        "\n",
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUSTOM LINEAR LAYER CODE FOR EFFICIENCY FROM LLAMA2 PAPER**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "by57ZKjzFkjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#custom linear for optimized forward and backward call\n",
        "class Custom_autograd_Linear(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input, weight, bias=None):\n",
        "    ctx.save_for_backward(input, weight, bias)\n",
        "    output = input @ weight.t()\n",
        "    if bias is not None:\n",
        "      output += bias\n",
        "    return output\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    input, weight, bias = ctx.saved_tensors\n",
        "    #print(\"Input shape:\", input.shape if input is not None else \"None\",  \"Weight shape:\", weight.shape if weight is not None else \"None\", \"grad_output shape:\", grad_output.shape if grad_output is not None else \"None\",\n",
        "          #\"Bias shape:\", bias.shape if bias is not None else \"None\")\n",
        "    grad_input = grad_weight = grad_bias = None\n",
        "    if ctx.needs_input_grad[0]:\n",
        "      grad_input = grad_output @ weight  #dl/dx  loss wrt input\n",
        "    if ctx.needs_input_grad[1]:\n",
        "      #grad_weight = grad_output.transpose(0, 1) @ input #dl/dw   B T C\n",
        "      grad_weight = torch.einsum('...o,...i->oi', grad_output, input)\n",
        "    if bias is not None and ctx.needs_input_grad[2]:\n",
        "      grad_bias = grad_output.sum(dim=[0, 1])  #dl/db\n",
        "    return grad_input, grad_weight, grad_bias\n",
        "\n",
        "class Optimize_linear(torch.nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
        "    if bias:\n",
        "      self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return Custom_autograd_Linear.apply(x, self.weight, self.bias if hasattr(self, 'bias') else None)"
      ],
      "metadata": {
        "id": "6ffoODY4A4dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it is important for efficient attention build by meta get recommendation from llama paper\n",
        "import xformers.ops as xops\n",
        "print(xops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3_rYOswX9yx",
        "outputId": "a2745c78-118f-4f23-aef4-49130f44438c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'xformers.ops' from '/usr/local/lib/python3.11/dist-packages/xformers/ops/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUNCTION OF FUNDAMENTAL UNIT OF MODEL 🧩**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0z3J6wbPF4D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int =  1024\n",
        "  vocab_size: int = 50257\n",
        "  n_layer: int = 12\n",
        "  n_head: int = 12\n",
        "  n_embd: int = 768\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "  def __init__(self,  dim, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(dim,))\n",
        "    self.eps = eps\n",
        "  def forward(self, x):\n",
        "    return x*torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)*self.gamma\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "class RoPE(torch.nn.Module):\n",
        "    def __init__(self, dim, base=10000, max_length=2048):\n",
        "      super().__init__()\n",
        "      assert dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
        "\n",
        "      self.dim = dim\n",
        "      self.base = base\n",
        "      self.max_length = max_length\n",
        "\n",
        "      theta = 1.0/base**(torch.arange(0, dim, 2)[: (self.dim // 2)].float()/dim)  #[: (self.dim // 2)] for security if it exceed more than self.dim//2 code taken from pytorch implementation of rope\n",
        "      self.register_buffer('theta', theta)\n",
        "\n",
        "      m = torch.arange(max_length, dtype=torch.float)\n",
        "      self.register_buffer('m',m)\n",
        "\n",
        "      #mtheta\n",
        "      mtheta = torch.outer(m, theta)\n",
        "      self.register_buffer('cos_val', torch.cos(mtheta))  #real part\n",
        "      self.register_buffer('sin_val', torch.sin(mtheta))  #imag part\n",
        "\n",
        "      #now I will combine cos sin such that at last dim it would look like [cos_val, sin_val] this code is good for intuition\n",
        "      cos_sin = torch.stack([self.cos_val, self.sin_val], dim=-1)\n",
        "      self.register_buffer('cos_sin', cos_sin)  #dim = [seq_len, dim/2,2]\n",
        "\n",
        "    def rotate_90(self, x):  #this rotate 90 resemble i*x in complex plane (i resemble iota)\n",
        "\n",
        "      #based on maths if the matrix is [x1, x2, x3, x4] then rotated matrix will be [-x2,x1, -x4,x3]\n",
        "      #this implementation i same as the RoPE implementation in hugging face\n",
        "      x1 = x[...,0::2]  #for taking even position\n",
        "      x2 = x[...,1::2] #for taking odd position\n",
        "      return torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
        "\n",
        "    def apply_rotatory(self, x, start_idx=0):\n",
        "      seq_len = x.size(2)\n",
        "\n",
        "      \"\"\"\n",
        "      Process2: with complex equation = rotated_x = cos(theta)*x + i*x*sin(theta)\n",
        "\n",
        "      cos_val = torch.stack((cos_val, cos_val), dim=-1).reshape(x.shape)\n",
        "      sin_val = torch.stack((sin_val, -sin_val), dim=-1).reshape(x.shape)\n",
        "      rotated_x = cos_val*x + self.rotate_90(x)*sin_val\n",
        "      return rotated_x\n",
        "\n",
        "      \"\"\"\n",
        "      x_reshaped = x.reshape(*x.shape[:-1], -1, 2)\n",
        "      \"\"\"\n",
        "      why we are doing this is more intuitive in coming code\n",
        "      we are making last dim as (dim/2, 2) ex: [x1,x2,x3,x4] as [[x1,x2],[x3,x4]] in last dim there will be one real part and one imaginary part\n",
        "      complex num: (a +ib) it will look like [a,b]\n",
        "      \"\"\"\n",
        "      x1 = x_reshaped[...,0] #this will take all the real part\n",
        "      x2 = x_reshaped[...,1] #... means all the dim same but from last dim only 1 index\n",
        "\n",
        "      original_x1 = x1.clone()\n",
        "\n",
        "\n",
        "      rope_cos_sin = self.cos_sin[start_idx: start_idx+seq_len].unsqueeze(0).unsqueeze(1)  #--> [seq_len, dim/2,2] ---> [1, 1,seq_len, dim/2,2]\n",
        "      cos_val = rope_cos_sin[...,0]\n",
        "      sin_val = rope_cos_sin[...,1]\n",
        "\n",
        "      \"\"\"\n",
        "      #other methos for cos and sin value but I think above one is more intuitive\n",
        "      cos_val = self.cos_val[start_idx: start_idx+seq_len].unsqueeze(0).unsqueeze(1).expand(x1.shape[0], x1.shape[1], -1, -1)\n",
        "      sin_val = self.sin_val[start_idx: start_idx+seq_len].unsqueeze(0).unsqueeze(1).expand(x1.shape[0], x1.shape[1], -1, -1)\n",
        "      # without expand it would look like  [1, 1, seq_len, dim//2]  but we are not using expand that gonna automatically broadcast and make the dim as [B,nh,seq_len, dim//2]\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      ## (a + bi)(cos θ + i sin θ) = (a cos θ - b sin θ) + (a sin θ + b cos θ)i  this is actual multiplication\n",
        "      x1 = x1*cos_val - x2*sin_val    #real part\n",
        "      x2 = original_x1*sin_val + x2*cos_val  #imaginary part\n",
        "\n",
        "      rotated_x = torch.stack([x1, x2], dim=-1).reshape(*x.shape)  #stack help to join real and img part it will join first index from x1 with first index from x2 to form again [x1,x2]\n",
        "\n",
        "      return rotated_x\n",
        "\n",
        "    def forward(self, q, k):\n",
        "      q = self.apply_rotatory(q)\n",
        "      k = self.apply_rotatory(k)\n",
        "      return q,k\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class swiGLU(torch.nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.linear = torch.nn.Linear(dim, 2*dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    input, gate = self.linear(x).chunk(2, dim=-1)  #diving a, 2b ---> a,b  as input and gate should  be independent and there parameter should also be different therefore it divide into 2\n",
        "    return input*torch.sigmoid(gate)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = RMSNorm(config.n_embd)\n",
        "    self.ln_2 = RMSNorm(config.n_embd)\n",
        "    self.attn = CasualAttention(config)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "#changing from nn.Linear to optimize_linear that i define\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.llama_config = int((2/3)*4*config.n_embd) #we use  this because instead of two matrics we gonna use three matrices in GLU variant FFN to make the parameter same as basemodel ref: SwiGlu paper 3.1\n",
        "    self.c_fc = Optimize_linear(config.n_embd, self.llama_config)\n",
        "    self.gelu = swiGLU(self.llama_config)  #nn.GELU(approximate='tanh') is replace\n",
        "    self.c_proj = Optimize_linear( self.llama_config, config.n_embd)\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.gelu(self.c_fc(x))\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "import xformers\n",
        "#xformers.set_log_level(\"DEBUG\")\n",
        "#import logging\n",
        "#logging.basicConfig(level=logging.DEBUG)   #to check whether xformer is using flashattention or not\n",
        "\n",
        "class CasualAttention(nn.Module):  #MultiHeadAttention\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self. c_attn = Optimize_linear(config.n_embd, 3*config.n_embd)  #general nn.Linear is replace for efficiency\n",
        "    self.c_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "    #self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    # Add RoPE\n",
        "    self.rope = RoPE(dim=config.n_embd //config.n_head, max_length=config.block_size)\n",
        "    self.register_buffer('attn_mask', xops.LowerTriangularMask().to(device))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T,C  = x.size()\n",
        "    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C//self.n_head).transpose(1, 2) #(B,T,C) ---> (B, T, nh, nc) --> (B, nh, T, nc)\n",
        "    k = k.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
        "\n",
        "    q, k = self.rope(q, k)\n",
        "\n",
        "    self.attn_mask = self.attn_mask.to(x.device)\n",
        "\n",
        "    #y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "    y = xops.memory_efficient_attention(q, k, v, attn_bias=self.attn_mask)\n",
        "    y = y.transpose(1,2).contiguous().view(B, T, C) # B nh T nc --> B T nc\n",
        "    #linear transformation for learning and compiling concat head in more correct order think like above y is concat opinion and after self.c_proj is like commeti that will take the comibined opinion and choose his opinion\n",
        "    #this is actually learned weight for better compiling learning\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "#MQA\n",
        "#this will mostly used at the last of pretaining and then in inference for fast , there is problem it reduce quality by a lot\n",
        "class MultiQueryAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.n_head = config.n_head\n",
        "    self.nc = self.n_embd // config.n_head\n",
        "\n",
        "    self.q_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.kv_proj = Optimize_linear(config.n_embd, 2*self.nc)   #this code reduce the parameters compare to first one\n",
        "    self.c_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "\n",
        "\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "\n",
        "    self.rope = RoPE(dim=config.n_embd //config.n_head, max_length=config.block_size)\n",
        "    self.register_buffer('attn_mask', xops.LowerTriangularMask().to(device))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T,C  = x.size()\n",
        "\n",
        "    q = self.q_proj(x)\n",
        "    k, v = self.kv_proj(x).split(self.nc, dim=2)\n",
        "\n",
        "    q = q.view(B, T, self.n_head, self.nc).transpose(1, 2) #(B,T,C) ---> (B, T, nh, nc) --> (B, nh, T, nc)\n",
        "    k = k.view(B, T, 1, self.nc).transpose(1, 2)\n",
        "    v = v.view(B, T, 1, self.nc).transpose(1, 2)\n",
        "\n",
        "    q, k = self.rope(q, k)\n",
        "    self.attn_mask = self.attn_mask.to(x.device)\n",
        "    y = xops.memory_efficient_attention(q, k, v, attn_bias=self.attn_mask)\n",
        "    y = y.transpose(1,2).contiguous().view(B, T, C) # B nh T nc --> B T nc\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "#MGA paper: GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints , this is actually generalized query attention and bet mha, mqa\n",
        "class GroupQueryAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_query_in_groups = 2\n",
        "    self.n_head = config.n_head\n",
        "    self.nc = self.n_embd // config.n_head\n",
        "    self.n_kv_heads = self.n_head // self.num_query_in_groups\n",
        "\n",
        "    self.q_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.kv_proj = Optimize_linear(config.n_embd, self.n_kv_heads*2*self.nc)  #initially there was 1 kv_heads here it depends on the number of query in the group\n",
        "\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "\n",
        "    self.rope = RoPE(dim=config.n_embd //config.n_head, max_length=config.block_size)\n",
        "    self.register_buffer('attn_mask', xops.LowerTriangularMask().to(device))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "\n",
        "    q = self.q_proj(x)\n",
        "    k, v = self.kv_proj(x).chunk(2, dim=-1)\n",
        "\n",
        "    q = q.view(B, T, self.n_head, self.nc).transpose(1, 2) #(B,T,C) ---> (B, T, nh, nc) --> (B, nh, T, nc)\n",
        "    k = k.view(B, T, self.n_kv_heads, self.nc).transpose(1, 2)  #(B, T, n_kv_head*nc) --> B, T, n_kv_head, nc)-->transpose--> B,n_kv_head, T, nc\n",
        "    v = v.view(B, T, self.n_kv_heads, self.nc).transpose(1, 2)\n",
        "\n",
        "    v = v.repeat_interleave(self.num_query_in_groups, dim=1)  #(B,n_kv_head, T, nc) --> (B,n_head, T, nc)  -- n_kv_head=n_head/num_query_in_groups\n",
        "    k = k.repeat_interleave(self.num_query_in_groups, dim=1)\n",
        "\n",
        "    q, k = self.rope(q, k)\n",
        "    self.attn_mask = self.attn_mask.to(x.device)\n",
        "    y = xops.memory_efficient_attention(q, k, v, attn_bias=self.attn_mask)\n",
        "    y = y.transpose(1,2).contiguous().view(B, T, C) # B nh T nc --> B T nc\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "      self.transformer = nn.ModuleDict(dict(\n",
        "          wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "          #wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "          h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "          ln_f = RMSNorm(config.n_embd)   #nn.LayerNorm(config.n_embd)  LLama replace layernorm with rmsNORM\n",
        "      ))\n",
        "      self.lm_head = Optimize_linear(config.n_embd, config.vocab_size, bias=False)\n",
        "      self.transformer.wte.weight = self.lm_head.weight #tied weight  sharing weight paper: Attention all you need and\n",
        "      #this saves a lot of computation around 40M paramater reduce\n",
        "\n",
        "      self.apply(self._init_weights)  #self.apply is a nn.module function that it iterate throuh all the self and use the function use self.apply(func)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "      \"\"\"\n",
        "             This code are just for testing what inside named_parameters\n",
        "              for pn, p in self.named_parameters(): #pn is name of the weight like  transformer.h.5.attn.c_attn.weight and p in the weight value\n",
        "                print('forloop', pn)\n",
        "      \"\"\"\n",
        "\n",
        "      if isinstance(module, Optimize_linear):  #here also I change from nn.Linear to cutom Optimize_linear\n",
        "        #print('module',module)\n",
        "        std = 0.02\n",
        "        if hasattr(module, 'SCALE_STD'):\n",
        "          #print('inside module',dir(module)) # good to see the  attribute inside the module\n",
        "          std *= (2*self.config.n_layer)**(-0.5)  #this are due to addition of residual x that was increasing std of the weight\n",
        "\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "        if hasattr(module, 'bias') and module.bias is not None: # Added check for bias existence\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      B, T = idx.size()\n",
        "      assert T <= self.config.block_size, f'Cannot forward T is greater than block_size: {T} > {self.config.block_size}'\n",
        "      token_embeddings = self.transformer.wte(idx) # B T\n",
        "      #position_embeddings = self.transformer.wpe(torch.arange(0, T,dtype=torch.long, device=idx.device)) # is removed in llama because rope is implemented in Attention where\n",
        "      # automatically position is included\n",
        "      x = token_embeddings #+ position_embeddings\n",
        "      for block in self.transformer.h:\n",
        "        x = checkpoint(block, x , use_reentrant=False)   #False tell you to use more efficient , optimised checkpoint\n",
        "      x = self.transformer.ln_f(x)\n",
        "      logits = self.lm_head(x)\n",
        "      loss=None\n",
        "      if targets is not None:\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
        "      return logits, loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, lr, device):\n",
        "      \"\"\"\n",
        "        weight decay is reguralization term used to penelized weight if it has higher value\n",
        "        loss = original loss + w_decay*sum|weight|^2 # the additional term force the weight to stay low\n",
        "      \"\"\"\n",
        "      param_dict = {pn:p for pn, p in self.named_parameters()}\n",
        "      param_dict = {pn:p for pn, p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "      decay_params = [p for n,p in param_dict.items() if p.dim()>=2]\n",
        "      nondecay_params = [p for n,p in param_dict.items() if p.dim()<2]\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nondecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "\n",
        "      num_decay_param = sum(p.numel() for p in decay_params)\n",
        "      num_nondecay_param = sum(p.numel() for p in nondecay_params)\n",
        "      print(f'len: {len(decay_params)} num_decay_param: {num_decay_param}')\n",
        "      print(f'len: {len(nondecay_params)} num_nondecay_param: {num_nondecay_param}')\n",
        "      #inspect.signature or __init__.__code__._co_varnames are use to find the constructor , arguments name in the class\n",
        "      #even __init__.__annotations__ this can be used\n",
        "      fused_available = 'fused' in torch.optim.AdamW.__init__.__code__.co_varnames #inspect.signature(torch.optim.AdamW).parameters I didnot use this as this has to use to install external library\n",
        "      used_fused = fused_available and device == 'cuda'\n",
        "      print(f'Using fused in Adam :{used_fused}')\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-5, fused=used_fused)\n",
        "      return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "jiFFKQS_M4j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL PLANTED 🌱**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NNZpL-y3Gx6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig(vocab_size=50304))  #making it good number\n",
        "print(model.__dict__)  #good away to know all the layer inside the model"
      ],
      "metadata": {
        "id": "bb72ZAjjNIdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b1c63aa-0a01-4ed3-dc5b-6533351eb238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'training': True, '_parameters': {}, '_buffers': {}, '_non_persistent_buffers_set': set(), '_backward_pre_hooks': OrderedDict(), '_backward_hooks': OrderedDict(), '_is_full_backward_hook': None, '_forward_hooks': OrderedDict(), '_forward_hooks_with_kwargs': OrderedDict(), '_forward_hooks_always_called': OrderedDict(), '_forward_pre_hooks': OrderedDict(), '_forward_pre_hooks_with_kwargs': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict(), '_load_state_dict_post_hooks': OrderedDict(), '_modules': {'transformer': ModuleDict(\n",
            "  (wte): Embedding(50304, 768)\n",
            "  (h): ModuleList(\n",
            "    (0-11): 12 x Block(\n",
            "      (ln_1): RMSNorm()\n",
            "      (ln_2): RMSNorm()\n",
            "      (attn): CasualAttention(\n",
            "        (c_attn): Optimize_linear()\n",
            "        (c_proj): Optimize_linear()\n",
            "        (rope): RoPE()\n",
            "      )\n",
            "      (mlp): MLP(\n",
            "        (c_fc): Optimize_linear()\n",
            "        (gelu): swiGLU(\n",
            "          (linear): Linear(in_features=2048, out_features=4096, bias=True)\n",
            "        )\n",
            "        (c_proj): Optimize_linear()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ln_f): RMSNorm()\n",
            "), 'lm_head': Optimize_linear()}, 'config': GPTConfig(block_size=1024, vocab_size=50304, n_layer=12, n_head=12, n_embd=768)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA 📚 LOADING FROM FINEWEB DEVELOPED BY HUGGING FACE 🤗**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N0_Sb93pEeBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2Bi7v1SqL2mK",
        "outputId": "d0e4cfff-f729-4858-fdcf-e589ff2366f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataload\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "#folder name\n",
        "local_dir = \"filter_fineweb5B\"\n",
        "#size of each file counted as token\n",
        "shard_size = int(1e8)\n",
        "\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(os.getcwd(), local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR , exist_ok=True)\n",
        "\n",
        "data_files = [f\"sample/10BT/{i:03d}_00000.parquet\" for i in range(2)]\n",
        "\n",
        "# Load only these specific files from the dataset\n",
        "fw = load_dataset(\n",
        "    \"HuggingFaceFW/fineweb-edu\",\n",
        "    data_files=data_files,\n",
        "    split=\"train\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWpGBRtmEcn1",
        "outputId": "ce7b1c41-cbc1-4620-a0d8-63152d930303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(doc):\n",
        "  enc = tiktoken.get_encoding(\"gpt2\")\n",
        "  eot = enc._special_tokens[\"<|endoftext|>\"]\n",
        "  tokens = [eot]\n",
        "  tokens += enc.encode(doc['text'], allowed_special={\"<|endoftext|>\"})\n",
        "  tokens_np = np.array(tokens, dtype=np.uint16)\n",
        "  #ensures that all token values are within the valid range for uint16.\n",
        "  assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"greater than 2**16 ie that is file is too big\"\n",
        "  return tokens_np\n",
        "\n",
        "def save_token_to_file(fileName, tokensNp):\n",
        "  np.save(fileName, tokensNp)\n",
        "\n",
        "#counting num if processor available for parellel computing or tokenizing\n",
        "nprocessors = max(1, int(os.cpu_count()/2) )\n",
        "print(f\"using {nprocessors} processes\")\n",
        "\n",
        "\n",
        "#process shards this code is more cleaner than andrej karpathy\n",
        "def process_shard(dataset, shard_size, DATA_CACHE_DIR, num_processes):\n",
        "  shard_index = 0\n",
        "  all_tokens = []\n",
        "  progress_bar = tqdm(total=shard_size, unit=\"token\", desc=f\"Writing shard {shard_index}\" )\n",
        "\n",
        "  def save_shard(tokens, split):\n",
        "    nonlocal shard_index\n",
        "    file_name = os.path.join(DATA_CACHE_DIR, f\"partialfineweb_{split}_{shard_index:04d}.npy\")\n",
        "    save_token_to_file(file_name, tokens)\n",
        "    shard_index += 1\n",
        "  #using muktiple processor to tokenize fast\n",
        "  with mp.Pool(processes=num_processes) as pool:\n",
        "    for tokens in pool.imap(tokenizer, dataset, chunksize=8):  # here each dataset is using tokensizer as a function a to convert the data\n",
        "        all_tokens.extend(tokens)\n",
        "        progress_bar.update(len(tokens))\n",
        "        # logic for saving shard little improvement then karpathy code\n",
        "        while len(all_tokens) > shard_size:\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            save_shard(all_tokens[:shard_size], split)\n",
        "            all_tokens = all_tokens[shard_size:]\n",
        "\n",
        "\n",
        "            progress_bar.reset(total=shard_size)\n",
        "            progress_bar.set_description(f\"Writing shard {shard_index}\", refresh=False )\n",
        "            # refresh=True then the laoding symbol will referesh again and again one below the other\n",
        "            # for me that is not clean to see that's why I use False it is clean and once .npy file\n",
        "            #is complete then only the next file can be visible\n",
        "\n",
        "    # Save any remaining tokens as the final shard\n",
        "    if all_tokens:\n",
        "        split = \"val\" if shard_index == 0 else \"train\"\n",
        "        save_shard(all_tokens, split)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeyOw1I5ETwK",
        "outputId": "6994fcca-ef39-44ca-81b7-950610cca60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using 1 processes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "#Dataloader for shakespare text\n",
        "class DataLoader:\n",
        "  def __init__(self, B, T, process_rank, num_processes):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    self.process_rank = process_rank\n",
        "    self.num_processes = num_processes\n",
        "\n",
        "    data = open('input.txt','r').read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(data)\n",
        "    self.tokens = torch.tensor(tokens, dtype= torch.long)\n",
        "    print(f'{len(self.tokens)} number of tokens loaded')\n",
        "    print(f'1 epoch is {len(self.tokens)//(self.B*self.T)} loop')\n",
        "\n",
        "    self.counter = self.B*self.T*self.process_rank\n",
        "\n",
        "  def next_batch(self):\n",
        "    b_tokens = self.tokens[self.counter : self.counter + (self.B*self.T)+1]\n",
        "    x = b_tokens[:-1].view(self.B, self.T)\n",
        "    y = b_tokens[1:].view(self.B, self.T)\n",
        "    self.counter += self.B*self.T*self.num_processes #here we advance due to num of GPU\n",
        "    if self.counter + (self.B*self.T*self.num_processes)+1 >= len(self.tokens):\n",
        "      self.counter = self.B*self.T*self.process_rank\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "# DataLoader for fineweb\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, batch_size, seq_length, process_rank, num_processes, split):\n",
        "        self.B = batch_size\n",
        "        self.T = seq_length\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {\"train\", \"val\"}, \"Split must be 'train' or 'val'\"\n",
        "\n",
        "        # Get shard filenames for the specified split\n",
        "        self.shards = [\n",
        "            os.path.join(DATA_CACHE_DIR, s)\n",
        "            for s in sorted(os.listdir(DATA_CACHE_DIR))\n",
        "            if split in s\n",
        "        ]\n",
        "        assert len(self.shards) > 0, f\"No shards found for {split} split\"\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_shard = 0\n",
        "        self.tokens = np.load(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank  # for process_rank 0 current positio will be 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]  # Get tokens for the batch\n",
        "        x = buf[:-1].reshape(B, T)  # Input tokens\n",
        "        y = buf[1:].reshape(B, T)  # Target tokens\n",
        "        self.current_position += B * T * self.num_processes\n",
        "\n",
        "        # If we reach the end of the current shard, move to the next shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            #we haven't use leftover tokens form the first shards because if we add this to the new added shard then it will currupt the file context meaning\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = np.load(self.shards[self.current_shard])  #loads a numpy file\n",
        "            self.current_position = B * T * self.process_rank\n",
        "\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "YonIbpNxNQRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 524288 #65536 #  #~0.5 M 2**19\n",
        "B = 8\n",
        "T = 1024  #context window\n",
        "assert batch_size % (B*T*ddp_world_size) == 0 , f'not divided check it'\n",
        "grad_accum = batch_size//(B*T*ddp_world_size)\n",
        "if master_process:\n",
        "   print(f' total batch size: {batch_size} |  gradient accumulation: {grad_accum}')\n",
        "print('I am GPU', ddp_local_rank)"
      ],
      "metadata": {
        "id": "FFjKvQAjNTnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a260fb80-a0b5-4a04-e16e-b18f53aa4d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " total batch size: 524288 |  gradient accumulation: 64\n",
            "I am GPU 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = sum(fw['token_count'])\n",
        "print(f'total number of tokens: {num_tokens/1e9:.2f} Billions\\n totals shards:{num_tokens/shard_size} ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8qH8y52NCH8",
        "outputId": "3ad00897-5703-4bbc-fcd9-eda35e453bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of tokens: 1.51 Billions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#process_shard means it will create a npy file each file will have shard_size{100M} tokens"
      ],
      "metadata": {
        "id": "Sr38-DIOVGjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_shard(fw, shard_size, DATA_CACHE_DIR, nprocessors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X-AAhkQXn3v",
        "outputId": "84f24c87-b9a7-4aca-cfe7-7c9b4ff23f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Writing shard 15:   3%|▎         | 3314602/100000000 [00:02<01:12, 1337739.63token/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the error I find here:\n",
        "* device mismatch of input and lowerTriangular matrices in casual attention\n",
        "* Error in implementing Custom Linear matrix multipication ka error"
      ],
      "metadata": {
        "id": "Cjubf_r3QEhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoaderLite(batch_size=B,seq_length=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='train')\n",
        "val_loader = DataLoaderLite(batch_size=B, seq_length=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='val')"
      ],
      "metadata": {
        "id": "enju0G6wFH0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HELLOSWAG EVALUATION 🤟**"
      ],
      "metadata": {
        "id": "IdvRycyVOsW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DKo1FnNd6tao",
        "outputId": "2934f9da-30db-48b9-e81b-e5127a9398e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"hellaswag\", split=\"validation\", trust_remote_code=True).select(range(10)) # trust_remote_code=True foe security purpose else it was showing error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "b1753bc7d61740069146779ae2635b32",
            "4d5849ebe8a241b4a1d3783a650fb9fb",
            "03fc89f217ed492887ff2bbe929dc583",
            "5e9e4d1843574218be5e2242aca70ae0",
            "e70700e607ac44ddb47f74384669fff4",
            "549e31ecdce14daeba8cebbcedcb12e9",
            "cfc0f601a17d40b3a4eae1266b57859f",
            "e98b8d70ceba44a2abd22048352bd437",
            "385c4d0f705943759fbbc26dc0ec2f80",
            "4c5d195f8cdc4e4d87bb86f9f34abd04",
            "a4951a39144d4d619d45af8c07df18fa",
            "1342ae08ef744e1abf14406fe10fe1e8",
            "0fb2b4308ee5488ebdf904cda012aa10",
            "0281101eb1d54df4b776266343a625be",
            "da6329d90e27475fa29c5a00158dddca",
            "0985979c2a1742c7bd70ff99e00e70e1",
            "6b7cfd6b5b654aa6a1e7f63e4989337d",
            "e3fea70996284e65b6ce94216013360d",
            "7232284f4d2049e0a2c2052bcbfece3d",
            "e9098cd144f748ee860d7161be37cf4c",
            "3805767a9cde4a0787c8ac5f5b201a93",
            "c49a4a47e4084546a6d669e9c1390255",
            "3dc74e64214845a39eae0a0620593235",
            "38500e391434470fbf10da7f24643904",
            "0fd0b93dbb444964855def2efbbf62ef",
            "8b8f5369d28f403889e0534b61b8dd35",
            "e6992589b6984f1b9689db532ffb6b9f",
            "b89b9bff5f8d44cb9e58790ba2c509dd",
            "c7de23614b4144308a6cd8c472a5676f",
            "5e001a40c3264f668eb7205beae3b125",
            "dcf017cf22d749c1b2f78c527317c046",
            "2016f006a79c4433986164efb4e97df1",
            "da81c658c3ad4183a7075509ec968a71",
            "57d2b5004caa4289b9a028c609a48f51",
            "8778048e3d6b4cb69c58ec308fcb83c6",
            "86f5ea248ea04272b94d1832075d66b6",
            "b0e6e37dcd4740fcb1a501c4ccbd82f2",
            "8737489066f04c3db5a5509dcca7a412",
            "6990515bef6e4f35a9b365625c177ffb",
            "f0ac60ce7c59471ab7bfd7a22bc753b3",
            "c59239ccc0c1429eb832e69d664ab5ff",
            "68bad90bf8d9460182a5f73e3dcfa571",
            "ea7914aa3c7d41dd90f0d5e438bf9d2a",
            "98ef4cc9e1b7426591b73c928b3e46b0",
            "3305e95044034f6ea903fca2956788a5",
            "fa78236aed7c4f58ab87ba69c0c56633",
            "1907649aa49f405c80d40492b123b1af",
            "a1c88925b0c84b9899724088c761c3b3",
            "b24c6e82f58f4418ad7681a1752cbc00",
            "6468c9f46ed4458bbafa298079cf985b",
            "e50364e0dd0c47c5aa9b2b3ced0e14e7",
            "d2c1a8beef8245b897a36a5454b5e7c6",
            "27146f936e4747ceb55f8722aa174985",
            "eceb511289584fb485ee37310a8800e8",
            "efd9471a5034441b8f514b7a37849429",
            "bfab93490b1144328eb3705d3c86418f",
            "46f5c65ef8004053b6c84963ef51c416",
            "cc78152ef350447485b4e4b4c79ed8f0",
            "c5975c9a807643fdb5c3fd3d3e93b6f5",
            "4fdec6411a2e41bba366a4e83f8f9e1a",
            "ccb7e6d540f74eb6a3275bf082daad41",
            "3bcb91091c6e4358b33e8ae17e3857d9",
            "e990e9f45e28455884f7e3b11e83f02e",
            "d77c496f2e0a41738ac7f93323ab5a34",
            "03744b4189be4a26bdf70ee41ce27591",
            "3b2c962470c64b6dbbc6993f6dd9cb8d"
          ]
        },
        "id": "paE0ZPBd60Zn",
        "outputId": "6d23ed8d-1d26-4ff4-c3b9-6e0e5eef728b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/47.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1753bc7d61740069146779ae2635b32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1342ae08ef744e1abf14406fe10fe1e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/12.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dc74e64214845a39eae0a0620593235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/39905 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57d2b5004caa4289b9a028c609a48f51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/10003 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3305e95044034f6ea903fca2956788a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/10042 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfab93490b1144328eb3705d3c86418f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "code for knowing the details of dataset\n",
        "\n",
        "print(len(dataset))\n",
        "print(\"--- Basic Dataset Information ---\")\n",
        "print(dataset[:2])\n",
        "print(\"\\n\")\n",
        "print(dataset.features)\n",
        "print(dataset.description)\n",
        "print(\"--- First 3 Examples ---\")\n",
        "for i in range(min(2, len(dataset))):\n",
        "    example = dataset[i]\n",
        "    print(f\"Example {i}:\")\n",
        "    for key, value in example.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"-\" * 20)\n",
        "print(\"\\n\")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5v2gXsd7yCO",
        "outputId": "ffc827da-d04d-418c-c533-3fb01ad9d38b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_hellaswag(dataset,model, device, max_seq_length=1024, ddp_world_size,ddp_rank):\n",
        "  \"\"\" this is more readable then andrej karpathy idea or concept I got from andrej the dl god  \"\"\"\n",
        "\n",
        "    # initialize tokenizer\n",
        "\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    pad_token = enc.eot_token  # Using EOS token for padding\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    num_total = 0 #this code is if you are using parrallel gpu then we are tracking example in each gpu else for one gpu you can direcly use len(dataset)\n",
        "    for i,example in enumerate(dataset):\n",
        "\n",
        "\n",
        "        if i% ddp_world_size != ddp_rank:\n",
        "          continue\n",
        "\n",
        "        \"\"\"\n",
        "            This is really important for gpu parrellism\n",
        "            if there are 4 gpu(ddp_world_size)\n",
        "            based on this code the example will distributed in each gpu\n",
        "            for ex:\n",
        "               if i=0 0%4 == 0 not equal 1,2,3\n",
        "               there except ddprank=0 in every other gpu continue will run and skip the rest code\n",
        "               only ddp_rank of 0 i.e. gpu 0 will run the code\n",
        "               in this way you can say for i=1 it will go to gpu 1\n",
        "        \"\"\"\n",
        "        context = example[\"ctx\"]\n",
        "        endings = example[\"endings\"]\n",
        "        label = int(example[\"label\"])\n",
        "\n",
        "        # Tokenize context once\n",
        "        context_tokens = enc.encode(context)\n",
        "        losses = []\n",
        "        max_combined_length = 0\n",
        "        combined_tokens = []\n",
        "        mask_tokens = []\n",
        "        for ending in endings:\n",
        "\n",
        "            ending_tokens = enc.encode(\" \" + ending)  # why use this ' ' because it's obvious after the last word in context there will be gap for sure\n",
        "\n",
        "            # Create combined sequence with padding\n",
        "            combined = context_tokens + ending_tokens\n",
        "\n",
        "            max_combined_length = max(max_combined_length, len(combined))\n",
        "            combined_tokens.append(combined)\n",
        "\n",
        "            mask = [0]*len(context_tokens) + [1]*len(ending_tokens)\n",
        "            mask_tokens.append(mask)\n",
        "\n",
        "        # to handle the limitation of model to take as input very rare but crucial case\n",
        "        if max_combined_length > max_seq_length:\n",
        "            # Truncate from left (beginning) to preserve ending\n",
        "            combined_tokens = [combined[-max_seq_length:] for combined in combined_tokens]\n",
        "            mask_tokens = [mask[-max_seq_length:] for mask in mask_tokens]\n",
        "            max_combined_length = max_seq_length\n",
        "\n",
        "        for token, mask in zip(combined_tokens, mask_tokens):\n",
        "           token += [pad_token] * (max_combined_length - len(token))\n",
        "           mask += [0] * (max_combined_length - len(mask))\n",
        "\n",
        "           input_tensor = torch.tensor(token[:-1], device=device).unsqueeze(0)\n",
        "           target_tensor = torch.tensor(token[1:], device=device).unsqueeze(0)\n",
        "           mask_tensor = torch.tensor(mask[1:], device=device).unsqueeze(0)  #mask should be same as target qki we have to use make in logits that should be same as y in dim\n",
        "           #and for loss of each token we gonna use target only therefore after calculating each lose we gonna remove loss value for ctx_token and padded token\n",
        "\n",
        "           with torch.no_grad():\n",
        "             logits, _ = model(input_tensor) #we are not using target because we gonna use target to calculate loss outside only why losse computation inside model function\n",
        "             loss_per_token = torch.nn.functional.cross_entropy(\n",
        "                 logits.view(-1, logits.shape[-1]),\n",
        "                 target_tensor.view(-1),\n",
        "                 reduction='none'\n",
        "             ).view_as(target_tensor)  # logits.view(-1, logits.shape[-1]) what this is makeing (B*T, C) #y,view(-1) bhi (B*T) we are trying to spread all the tokens in single line so that we can compare logits with target one by one and calculate loss\n",
        "             masked_loss = (loss_per_token*mask_tensor).mean()\n",
        "             losses.append(masked_loss.item() if not torch.isnan(masked_loss) else float('inf'))\n",
        "\n",
        "        if losses:\n",
        "            prediction = np.nanargmin(losses)\n",
        "            correct += int(prediction == label)\n",
        "            num_total += 1\n",
        "\n",
        "    if ddp_world_size > 1 and ddp_rank is not None:\n",
        "        num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
        "        correct = torch.tensor(correct, dtype=torch.long, device=device)\n",
        "        dist.all_reduce(correct, op=dist.ReduceOp.SUM)\n",
        "        dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
        "        num_total = num_total.item()\n",
        "        correct = correct.item()\n",
        "\n",
        "    accuracy = correct / num_total if num_total > 0 else 0.0 #to avoid divide by zero\n",
        "    accuracy = round(accuracy, 2)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QTkm8ATws14M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING MODEL 💃**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yBHyHUlNFKAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#converting to tf32 from fp32\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "raw_model = model\n",
        "raw_model.to(device)\n",
        "\n",
        "if ddp:\n",
        "  model = torch.nn.parallel.DistributedDataParallel(raw_model, device_ids=[ddp_local_rank])\n",
        "\n",
        "raw_model = model.module if ddp else raw_model # get unwrapped model if DDP\n",
        "\n",
        "\n",
        "\n",
        "model = torch.compile(raw_model)\n",
        "\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr*0.1\n",
        "warmup_steps = 10  #2000 for llama 2\n",
        "max_steps = 50\n",
        "\n",
        "def get_lr(it):\n",
        "  if it < warmup_steps:\n",
        "    return max_lr*(it+1)/warmup_steps\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "\n",
        "  decay_ratio = (it - warmup_steps)/(max_steps - warmup_steps)\n",
        "  coeff = 0.5*(1.0 + math.cos(math.pi*decay_ratio))\n",
        "  return min_lr + coeff*(max_lr - min_lr)\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "print(f\"Type of raw_model: {type(model)}\")\n",
        "#optimizer\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, lr=6e-4, device=device)\n",
        "\n",
        "\n",
        "#log directory to write helloswag and loss value\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: #open for writing for clearing the file\n",
        "  pass\n",
        "total_steps = 20\n",
        "\n",
        "for i in range(total_steps):\n",
        "      t0 = time.time()\n",
        "\n",
        "      #val loss  same as training below code without validation\n",
        "      if step%10 == 0:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "          val_loss_accum2 = 0.0\n",
        "          val_loss_steps = 20\n",
        "          for _ in range(val_loss_steps):\n",
        "            x,y = train_loader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(x, y)\n",
        "            loss = loss/val_loss_steps\n",
        "            val_loss_accum2 += loss.detach()\n",
        "          if ddp:\n",
        "            # Gradient synchronization (all_reduce) happens here for accumulated gradients\n",
        "            torch.distributed.all_reduce(val_loss_accum2, op=torch.distributed.ReduceOp.AVG)\n",
        "          if master_process:\n",
        "            print(f'val loss: {val_loss_accum2.item():.4f} ')\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "\n",
        "      #sample code once in a while\n",
        "\n",
        "      if step%10 == 0  and False:\n",
        "          model.eval()\n",
        "          num_return_sequences = 3\n",
        "          max_length = 30\n",
        "          enc = tiktoken.get_encoding('gpt2')\n",
        "          tokens = enc.encode('Hello, Why I am thinking')\n",
        "          tokens = torch.tensor(tokens, dtype=torch.long) #(8,)\n",
        "          tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "          x  = tokens.to(device)\n",
        "          sample_rng = torch.Generator(device=device)\n",
        "          sample_rng.manual_seed(42+ddp_rank)\n",
        "          torch.cuda.manual_seed(42)\n",
        "          while x.size(1) < max_length:\n",
        "            with torch.no_grad():\n",
        "              logits, loss = model(x) #(B, T, vocal_size)\n",
        "              #print('logits1', logits1.shape)\n",
        "              logits = logits[:, -1, :]   #(B,vocab_size)\n",
        "              #print('logits1', logits.shape)\n",
        "              probs = F.softmax(logits, dim=-1)\n",
        "              #print('probs', probs.shape)\n",
        "\n",
        "              topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) #we are selecting top 50 along last dim #(B,50)\n",
        "              #print('topk_probs', topk_probs.shape)\n",
        "              next_token = torch.multinomial( topk_probs, num_samples=1,generator=sample_rng,  replacement=True) #(B, 1)\n",
        "              xcol = torch.gather(topk_indices, -1, next_token) #(B, 1)\n",
        "              x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "          for i in range(num_return_sequences):\n",
        "            tokens = x[i,:max_length].tolist()\n",
        "            decode = enc.decode(tokens)\n",
        "            print(f'sequence {i}: {decode}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      loss_accum = 0.0\n",
        "      for j in range(grad_accum):\n",
        "        x,y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        #with torch.autocast(device_type=device, dtype=torch.bfloat16, enabled=False):  #mixed precision where all the other weight are fp32 only the logits are in bfoat16\n",
        "          #in tesla gpu this slow the process TESLA T4 doesn't support bfloat16\n",
        "        logits, loss = model(x, y)\n",
        "          #import code; code.interact(local=locals())\n",
        "        loss = loss/grad_accum\n",
        "        loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "          # Control gradient synchronization for overlap\n",
        "          model.require_backward_grad_sync = (j == grad_accum - 1)\n",
        "        loss.backward()\n",
        "      if ddp:\n",
        "        # Gradient synchronization (all_reduce) happens here for accumulated gradients\n",
        "        torch.distributed.all_reduce(loss_accum, op=torch.distributed.ReduceOp.AVG)  #all the loss in the gpu get accumulated so that we can get the right loss value in the print else it gonna print the loss in the master process only\n",
        "\n",
        "      #gradient clipping\n",
        "      norm = torch.nn.utils.clip_grad_norm_(raw_model.parameters(), 1.0) #this is actually rms of grad if it goes up it is bad for the model\n",
        "      # if the rms of weight exceed 1 then it will clip value\n",
        "      lr = get_lr(i)\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "      optimizer.step()\n",
        "      torch.cuda.synchronize()\n",
        "      t1 = time.time()\n",
        "      dt = (t1 - t0)*1000\n",
        "      tokens_processed = train_loader.B * train_loader.T*grad_accum*ddp_world_size\n",
        "      tokens_per_second = tokens_processed / (t1 - t0)\n",
        "      if master_process:\n",
        "        print(f' step {i:2d} | loss: {loss_accum.item():.4f} | lr: {lr:.4f} | norm: {norm:.4f} |  dt: {dt:.2f}ms ({tokens_per_second:.2f} tok/s) ')\n",
        "        #saving value of loss function\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"step {i} train {loss_accum.item():.6f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "vzSmhDgheMGs",
        "outputId": "d8bbb2ed-4d43-4d03-822c-dfed542fd37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of raw_model: <class 'torch._dynamo.eval_frame.OptimizedModule'>\n",
            "len: 61 num_decay_param: 205357056\n",
            "len: 85 num_nondecay_param: 139008\n",
            "Using fused in Adam :True\n",
            " step  0 | loss: 10.9854 | lr: 0.0001 | norm: 0.1057 |  dt: 237137.50ms (2210.90 tok/s) \n",
            " step  1 | loss: 10.6800 | lr: 0.0001 | norm: 0.0962 |  dt: 249075.29ms (2104.94 tok/s) \n",
            " step  2 | loss: 10.3496 | lr: 0.0002 | norm: 0.1031 |  dt: 250323.90ms (2094.44 tok/s) \n",
            " step  3 | loss: 10.0749 | lr: 0.0002 | norm: 0.1724 |  dt: 251046.16ms (2088.41 tok/s) \n",
            " step  4 | loss: 9.9084 | lr: 0.0003 | norm: 0.2894 |  dt: 250196.92ms (2095.50 tok/s) \n",
            " step  5 | loss: 10.0120 | lr: 0.0004 | norm: 0.4252 |  dt: 250135.38ms (2096.02 tok/s) \n",
            " step  6 | loss: 9.9951 | lr: 0.0004 | norm: 0.2688 |  dt: 250503.18ms (2092.94 tok/s) \n",
            " step  7 | loss: 9.8280 | lr: 0.0005 | norm: 0.2614 |  dt: 250127.04ms (2096.09 tok/s) \n",
            " step  8 | loss: 9.7289 | lr: 0.0005 | norm: 0.1888 |  dt: 250116.30ms (2096.18 tok/s) \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-26f34326bf82>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_accum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m    \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m    \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m    \u001b[0;31m#with torch.autocast(device_type=device, dtype=torch.bfloat16, enabled=False):  #mixed precision where all the other weight are fp32 only the logits are in bfoat16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7Zu56_WNUcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFERENCING CODE 📝**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mPyZQfx1Hzt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 3\n",
        "max_length = 30\n",
        "model.eval()\n",
        "model.to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode('Hello, I am a bad')\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8,)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x  = tokens.to('cuda')\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits, loss = model(x)\n",
        "    #print('logits1', logits1.shape)\n",
        "    logits = logits[:, -1, :]\n",
        "    #print('logits1', logits.shape)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    #print('probs', probs.shape)\n",
        "\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) #we are selecting top 50 along last dim\n",
        "    #print('topk_probs', topk_probs.shape)\n",
        "    next_token = torch.multinomial( topk_probs, num_samples=1, replacement=True)\n",
        "    xcol = torch.gather(topk_indices, -1, next_token)\n",
        "    x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i,:max_length].tolist()\n",
        "  decode = enc.decode(tokens)\n",
        "  print(f'sequence {i}: {decode}')"
      ],
      "metadata": {
        "id": "Yu7iuCtcNovv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5a63c4-702d-47eb-99bb-c29760dd97ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequence 0: Hello, I am a bad the the the the. 2013 to the the the the the the the the and\n",
            ", the,.\n",
            " the and\n",
            "sequence 1: Hello, I am a bad. dreadful the the the the aisch. This sqor Perform.と the, and The the. astonishing, the the\n",
            "sequence 2: Hello, I am a bad. Theophical the the the the the the the the the in a Quantum sqoramps. The the the the. Buck\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "57ziKwJaPozK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}