# 204M_tiny_Llama2_build_from_scratch
This is llama2 model code build from scratch using pytorch. 204M parameter. I have write the code to train the model on 1.5 Billion tokens but you can change the tokens as per your wish.

There are some code to update
* Include helloswag inside training loop
*  saving checkpoint of the model after certain loop

  ![image](https://github.com/user-attachments/assets/b3b471d4-f750-430b-97e9-ff9dfdb7033e)

