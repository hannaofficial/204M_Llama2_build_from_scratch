{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+prDobC6beF/G9IOiJON+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannaofficial/204M_Llama2_build_from_scratch/blob/main/llama2_built_from_scratch_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did I write this code\n",
        "\n",
        "I take GPT3 code as reference that I learn from adrej karpathy and then I modified the code based on llama research paper llama 1,2,3\n",
        "\n",
        "Only difference bet llama 2 and 3 is scaling(50x more parameter than llama2) but all the fundamental unit are moreover same that's why I write llama2 not 3.\n",
        "\n",
        "Even my model is tiny."
      ],
      "metadata": {
        "id": "G2LA-RbEENN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the diiference in LLama wrt gpt3\n",
        "\n",
        "* RMS normalization compare to layernorm\n",
        "* Rope with respected to static position embedding with training parameter\n",
        "* Swiglu wrt Gelu\n",
        "* implementing xformers instead of Flash attention it is same but more efficient then direct implementation and also use the latest top attention\n",
        "* custom linear implemention and save checkpoint for ram efficiency"
      ],
      "metadata": {
        "id": "biIj49jzRgza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install triton    #xformers is install due to implement efficient flash attention develop by meta\n",
        "!pip install xformers\n",
        "!pip install tiktoken\n",
        "!pip install xformers --upgrade"
      ],
      "metadata": {
        "id": "GDup2riwLNR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYPvoemh32xi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498c2bf6-4859-4c2f-8d01-9a1b44aa46b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda is available\n",
            "using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "import inspect\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "print(f'{device} is available')\n",
        "\n",
        "\n",
        "#----------------------------------------------------------\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "\n",
        "#set up DDP (distributed data parrallel)\n",
        "#trochrun command set the env varable RANK, LOCALRANK and WORLD SIZE\n",
        "ddp = int(os.environ.get('RANK',-1)) != -1\n",
        "if ddp:\n",
        "  assert torch.cuda.is_available(), 'for now i think we need cuda'\n",
        "  init_process_group(backend='nccl')\n",
        "  ddp_rank = int(os.environ['RANK'])  #identification of each gpu\n",
        "  ddp_local_rank = int(os.environ['LOCAL_RANK'])  #rank of the gpu on single node\n",
        "  ddp_world_size = int(os.environ['WORLD_SIZE'])  #total num of processes/gpu running\n",
        "  device = f'cuda:{ddp_local_rank}'\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank == 0\n",
        "else:\n",
        "  #vanilla, non-DDP run\n",
        "  ddp_rank = 0\n",
        "  ddp_local_rank = 0\n",
        "  ddp_world_size = 1\n",
        "  master_process = True\n",
        "  #attempt to autodetect device\n",
        "  device = \"cpu\"\n",
        "  if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "  elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "  print(f\"using device: {device}\")\n",
        "\n",
        "#------------------------------\n",
        "\n",
        "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CUSTOM LINEAR LAYER CODE FOR EFFICIENCY FROM LLAMA2 PAPER**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "by57ZKjzFkjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#custom linear for optimized forward and backward call\n",
        "class Custom_autograd_Linear(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, input, weight, bias=None):\n",
        "    ctx.save_for_backward(input, weight, bias)\n",
        "    output = input @ weight.t()\n",
        "    if bias is not None:\n",
        "      output += bias\n",
        "    return output\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    input, weight, bias = ctx.saved_tensors\n",
        "    #print(\"Input shape:\", input.shape if input is not None else \"None\",  \"Weight shape:\", weight.shape if weight is not None else \"None\", \"grad_output shape:\", grad_output.shape if grad_output is not None else \"None\",\n",
        "          #\"Bias shape:\", bias.shape if bias is not None else \"None\")\n",
        "    grad_input = grad_weight = grad_bias = None\n",
        "    if ctx.needs_input_grad[0]:\n",
        "      grad_input = grad_output @ weight  #dl/dx  loss wrt input\n",
        "    if ctx.needs_input_grad[1]:\n",
        "      #grad_weight = grad_output.transpose(0, 1) @ input #dl/dw   B T C\n",
        "      grad_weight = torch.einsum('...o,...i->oi', grad_output, input)\n",
        "    if bias is not None and ctx.needs_input_grad[2]:\n",
        "      grad_bias = grad_output.sum(dim=[0, 1])  #dl/db\n",
        "    return grad_input, grad_weight, grad_bias\n",
        "\n",
        "class Optimize_linear(torch.nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias=True):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
        "    if bias:\n",
        "      self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    return Custom_autograd_Linear.apply(x, self.weight, self.bias if hasattr(self, 'bias') else None)"
      ],
      "metadata": {
        "id": "6ffoODY4A4dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#it is important for efficient attention build by meta get recommendation from llama paper\n",
        "import xformers.ops as xops\n",
        "print(xops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3_rYOswX9yx",
        "outputId": "a2745c78-118f-4f23-aef4-49130f44438c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'xformers.ops' from '/usr/local/lib/python3.11/dist-packages/xformers/ops/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUNCTION OF FUNDAMENTAL UNIT OF MODEL 🧩**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0z3J6wbPF4D-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int =  1024\n",
        "  vocab_size: int = 50257\n",
        "  n_layer: int = 12\n",
        "  n_head: int = 12\n",
        "  n_embd: int = 768\n",
        "\n",
        "class RMSNorm(torch.nn.Module):\n",
        "  def __init__(self,  dim, eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(dim,))\n",
        "    self.eps = eps\n",
        "  def forward(self, x):\n",
        "    return x*torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)*self.gamma\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "class RoPE(torch.nn.Module):\n",
        "    def __init__(self, dim, base=10000, max_length=2048):\n",
        "      super().__init__()\n",
        "      assert dim % 2 == 0, \"Dimension must be divisible by 2\"\n",
        "\n",
        "      self.dim = dim\n",
        "      self.base = base\n",
        "      self.max_length = max_length\n",
        "\n",
        "      theta = 1.0/base**(torch.arange(0, dim, 2)[: (self.dim // 2)].float()/dim)  #[: (self.dim // 2)] for security if it exceed more than self.dim//2 code taken from pytorch implementation of rope\n",
        "      self.register_buffer('theta', theta)\n",
        "\n",
        "      m = torch.arange(max_length, dtype=torch.float)\n",
        "      self.register_buffer('m',m)\n",
        "\n",
        "      #mtheta\n",
        "      mtheta = torch.outer(m, theta)\n",
        "      self.register_buffer('cos_val', torch.cos(mtheta))  #real part\n",
        "      self.register_buffer('sin_val', torch.sin(mtheta))  #imag part\n",
        "\n",
        "      #now I will combine cos sin such that at last dim it would look like [cos_val, sin_val] this code is good for intuition\n",
        "      cos_sin = torch.stack([self.cos_val, self.sin_val], dim=-1)\n",
        "      self.register_buffer('cos_sin', cos_sin)  #dim = [seq_len, dim/2,2]\n",
        "\n",
        "    def rotate_90(self, x):  #this rotate 90 resemble i*x in complex plane (i resemble iota)\n",
        "\n",
        "      #based on maths if the matrix is [x1, x2, x3, x4] then rotated matrix will be [-x2,x1, -x4,x3]\n",
        "      #this implementation i same as the RoPE implementation in hugging face\n",
        "      x1 = x[...,0::2]  #for taking even position\n",
        "      x2 = x[...,1::2] #for taking odd position\n",
        "      return torch.stack((-x2, x1), dim=-1).reshape_as(x)\n",
        "\n",
        "    def apply_rotatory(self, x, start_idx=0):\n",
        "      seq_len = x.size(2)\n",
        "\n",
        "      \"\"\"\n",
        "      Process2: with complex equation = rotated_x = cos(theta)*x + i*x*sin(theta)\n",
        "\n",
        "      cos_val = torch.stack((cos_val, cos_val), dim=-1).reshape(x.shape)\n",
        "      sin_val = torch.stack((sin_val, -sin_val), dim=-1).reshape(x.shape)\n",
        "      rotated_x = cos_val*x + self.rotate_90(x)*sin_val\n",
        "      return rotated_x\n",
        "\n",
        "      \"\"\"\n",
        "      x_reshaped = x.reshape(*x.shape[:-1], -1, 2)\n",
        "      \"\"\"\n",
        "      why we are doing this is more intuitive in coming code\n",
        "      we are making last dim as (dim/2, 2) ex: [x1,x2,x3,x4] as [[x1,x2],[x3,x4]] in last dim there will be one real part and one imaginary part\n",
        "      complex num: (a +ib) it will look like [a,b]\n",
        "      \"\"\"\n",
        "      x1 = x_reshaped[...,0] #this will take all the real part\n",
        "      x2 = x_reshaped[...,1] #... means all the dim same but from last dim only 1 index\n",
        "\n",
        "      original_x1 = x1.clone()\n",
        "\n",
        "\n",
        "      rope_cos_sin = self.cos_sin[start_idx: start_idx+seq_len].unsqueeze(0).unsqueeze(1)  #--> [seq_len, dim/2,2] ---> [1, 1,seq_len, dim/2,2]\n",
        "      cos_val = rope_cos_sin[...,0]\n",
        "      sin_val = rope_cos_sin[...,1]\n",
        "\n",
        "      \"\"\"\n",
        "      #other methos for cos and sin value but I think above one is more intuitive\n",
        "      cos_val = self.cos_val[start_idx: start_idx+seq_len].unsqueeze(0).unsqueeze(1).expand(x1.shape[0], x1.shape[1], -1, -1)\n",
        "      sin_val = self.sin_val[start_idx: start_idx+seq_len].unsqueeze(0).unsqueeze(1).expand(x1.shape[0], x1.shape[1], -1, -1)\n",
        "      # without expand it would look like  [1, 1, seq_len, dim//2]  but we are not using expand that gonna automatically broadcast and make the dim as [B,nh,seq_len, dim//2]\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      ## (a + bi)(cos θ + i sin θ) = (a cos θ - b sin θ) + (a sin θ + b cos θ)i  this is actual multiplication\n",
        "      x1 = x1*cos_val - x2*sin_val    #real part\n",
        "      x2 = original_x1*sin_val + x2*cos_val  #imaginary part\n",
        "\n",
        "      rotated_x = torch.stack([x1, x2], dim=-1).reshape(*x.shape)  #stack help to join real and img part it will join first index from x1 with first index from x2 to form again [x1,x2]\n",
        "\n",
        "      return rotated_x\n",
        "\n",
        "    def forward(self, q, k):\n",
        "      q = self.apply_rotatory(q)\n",
        "      k = self.apply_rotatory(k)\n",
        "      return q,k\n",
        "\n",
        "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "class swiGLU(torch.nn.Module):\n",
        "  def __init__(self, dim):\n",
        "    super().__init__()\n",
        "    self.dim = dim\n",
        "    self.linear = torch.nn.Linear(dim, 2*dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    input, gate = self.linear(x).chunk(2, dim=-1)  #diving a, 2b ---> a,b  as input and gate should  be independent and there parameter should also be different therefore it divide into 2\n",
        "    return input*torch.sigmoid(gate)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = RMSNorm(config.n_embd)\n",
        "    self.ln_2 = RMSNorm(config.n_embd)\n",
        "    self.attn = CasualAttention(config)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "#changing from nn.Linear to optimize_linear that i define\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.llama_config = int((2/3)*4*config.n_embd) #we use  this because instead of two matrics we gonna use three matrices in GLU variant FFN to make the parameter same as basemodel ref: SwiGlu paper 3.1\n",
        "    self.c_fc = Optimize_linear(config.n_embd, self.llama_config)\n",
        "    self.gelu = swiGLU(self.llama_config)  #nn.GELU(approximate='tanh') is replace\n",
        "    self.c_proj = Optimize_linear( self.llama_config, config.n_embd)\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.gelu(self.c_fc(x))\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "import xformers\n",
        "#xformers.set_log_level(\"DEBUG\")\n",
        "#import logging\n",
        "#logging.basicConfig(level=logging.DEBUG)   #to check whether xformer is using flashattention or not\n",
        "\n",
        "class CasualAttention(nn.Module):  #MultiHeadAttention\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self. c_attn = Optimize_linear(config.n_embd, 3*config.n_embd)  #general nn.Linear is replace for efficiency\n",
        "    self.c_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "    #self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    # Add RoPE\n",
        "    self.rope = RoPE(dim=config.n_embd //config.n_head, max_length=config.block_size)\n",
        "    self.register_buffer('attn_mask', xops.LowerTriangularMask().to(device))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T,C  = x.size()\n",
        "    q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "    q = q.view(B, T, self.n_head, C//self.n_head).transpose(1, 2) #(B,T,C) ---> (B, T, nh, nc) --> (B, nh, T, nc)\n",
        "    k = k.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
        "    v = v.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
        "\n",
        "    q, k = self.rope(q, k)\n",
        "\n",
        "    self.attn_mask = self.attn_mask.to(x.device)\n",
        "\n",
        "    #y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "    y = xops.memory_efficient_attention(q, k, v, attn_bias=self.attn_mask)\n",
        "    y = y.transpose(1,2).contiguous().view(B, T, C) # B nh T nc --> B T nc\n",
        "    #linear transformation for learning and compiling concat head in more correct order think like above y is concat opinion and after self.c_proj is like commeti that will take the comibined opinion and choose his opinion\n",
        "    #this is actually learned weight for better compiling learning\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "#MQA\n",
        "#this will mostly used at the last of pretaining and then in inference for fast , there is problem it reduce quality by a lot\n",
        "class MultiQueryAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.n_head = config.n_head\n",
        "    self.nc = self.n_embd // config.n_head\n",
        "\n",
        "    self.q_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.kv_proj = Optimize_linear(config.n_embd, 2*self.nc)   #this code reduce the parameters compare to first one instead of n_embd we use self.nc\n",
        "    self.c_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "\n",
        "\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "\n",
        "    self.rope = RoPE(dim=config.n_embd //config.n_head, max_length=config.block_size)\n",
        "    self.register_buffer('attn_mask', xops.LowerTriangularMask().to(device))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T,C  = x.size()\n",
        "\n",
        "    q = self.q_proj(x)\n",
        "    k, v = self.kv_proj(x).split(self.nc, dim=2)\n",
        "\n",
        "    q = q.view(B, T, self.n_head, self.nc).transpose(1, 2) #(B,T,C) ---> (B, T, nh, nc) --> (B, nh, T, nc)\n",
        "    k = k.view(B, T, 1, self.nc).transpose(1, 2)\n",
        "    v = v.view(B, T, 1, self.nc).transpose(1, 2)\n",
        "\n",
        "    q, k = self.rope(q, k)\n",
        "    self.attn_mask = self.attn_mask.to(x.device)\n",
        "    y = xops.memory_efficient_attention(q, k, v, attn_bias=self.attn_mask)\n",
        "    y = y.transpose(1,2).contiguous().view(B, T, C) # B nh T nc --> B T nc\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "#MGA paper: GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints , this is actually generalized query attention and bet mha, mqa\n",
        "class GroupQueryAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_query_in_groups = 2\n",
        "    self.n_head = config.n_head\n",
        "    self.nc = self.n_embd // config.n_head\n",
        "    self.n_kv_heads = self.n_head // self.num_query_in_groups\n",
        "\n",
        "    self.q_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.kv_proj = Optimize_linear(config.n_embd, self.n_kv_heads*2*self.nc)  #initially there was 1 kv_heads here it depends on the number of query in the group\n",
        "\n",
        "    self.n_embd = config.n_embd\n",
        "    self.c_proj = Optimize_linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.SCALE_STD = 1.0\n",
        "\n",
        "    self.rope = RoPE(dim=config.n_embd //config.n_head, max_length=config.block_size)\n",
        "    self.register_buffer('attn_mask', xops.LowerTriangularMask().to(device))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size()\n",
        "\n",
        "    q = self.q_proj(x)\n",
        "    k, v = self.kv_proj(x).chunk(2, dim=-1)\n",
        "\n",
        "    q = q.view(B, T, self.n_head, self.nc).transpose(1, 2) #(B,T,C) ---> (B, T, nh, nc) --> (B, nh, T, nc)\n",
        "    k = k.view(B, T, self.n_kv_heads, self.nc).transpose(1, 2)  #(B, T, n_kv_head*nc) --> B, T, n_kv_head, nc)-->transpose--> B,n_kv_head, T, nc\n",
        "    v = v.view(B, T, self.n_kv_heads, self.nc).transpose(1, 2)\n",
        "\n",
        "    v = v.repeat_interleave(self.num_query_in_groups, dim=1)  #(B,n_kv_head, T, nc) --> (B,n_head, T, nc)  -- n_kv_head=n_head/num_query_in_groups #har ek v or k double(only here qki no of query in groups 2 diya) ho jayega\n",
        "    k = k.repeat_interleave(self.num_query_in_groups, dim=1)\n",
        "\n",
        "    q, k = self.rope(q, k)\n",
        "    self.attn_mask = self.attn_mask.to(x.device)\n",
        "    y = xops.memory_efficient_attention(q, k, v, attn_bias=self.attn_mask)\n",
        "    y = y.transpose(1,2).contiguous().view(B, T, C) # B nh T nc --> B T nc\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "      super().__init__()\n",
        "      self.config = config\n",
        "      self.transformer = nn.ModuleDict(dict(\n",
        "          wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "          #wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "          h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "          ln_f = RMSNorm(config.n_embd)   #nn.LayerNorm(config.n_embd)  LLama replace layernorm with rmsNORM\n",
        "      ))\n",
        "      self.lm_head = Optimize_linear(config.n_embd, config.vocab_size, bias=False)\n",
        "      self.transformer.wte.weight = self.lm_head.weight #tied weight  sharing weight paper: Attention all you need and\n",
        "      #this saves a lot of computation around 40M paramater reduce\n",
        "\n",
        "      self.apply(self._init_weights)  #self.apply is a nn.module function that it iterate throuh all the self and use the function use self.apply(func)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "      \"\"\"\n",
        "             This code are just for testing what inside named_parameters\n",
        "              for pn, p in self.named_parameters(): #pn is name of the weight like  transformer.h.5.attn.c_attn.weight and p in the weight value\n",
        "                print('forloop', pn)\n",
        "      \"\"\"\n",
        "\n",
        "      if isinstance(module, Optimize_linear):  #here also I change from nn.Linear to cutom Optimize_linear\n",
        "        #print('module',module)\n",
        "        std = 0.02\n",
        "        if hasattr(module, 'SCALE_STD'):\n",
        "          #print('inside module',dir(module)) # good to see the  attribute inside the module\n",
        "          std *= (2*self.config.n_layer)**(-0.5)  #this are due to addition of residual x that was increasing std of the weight\n",
        "\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "\n",
        "        if hasattr(module, 'bias') and module.bias is not None: # Added check for bias existence\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module, nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "      B, T = idx.size()\n",
        "      assert T <= self.config.block_size, f'Cannot forward T is greater than block_size: {T} > {self.config.block_size}'\n",
        "      token_embeddings = self.transformer.wte(idx) # B T\n",
        "      #position_embeddings = self.transformer.wpe(torch.arange(0, T,dtype=torch.long, device=idx.device)) # is removed in llama because rope is implemented in Attention where\n",
        "      # automatically position is included\n",
        "      x = token_embeddings #+ position_embeddings\n",
        "      for block in self.transformer.h:\n",
        "        x = checkpoint(block, x , use_reentrant=False)   #False tell you to use more efficient , optimised checkpoint\n",
        "      x = self.transformer.ln_f(x)\n",
        "      logits = self.lm_head(x)\n",
        "      loss=None\n",
        "      if targets is not None:\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), targets.view(-1))\n",
        "      return logits, loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, lr, device):\n",
        "      \"\"\"\n",
        "        weight decay is reguralization term used to penelized weight if it has higher value\n",
        "        loss = original loss + w_decay*sum|weight|^2 # the additional term force the weight to stay low\n",
        "      \"\"\"\n",
        "      param_dict = {pn:p for pn, p in self.named_parameters()}\n",
        "      param_dict = {pn:p for pn, p in param_dict.items() if p.requires_grad}\n",
        "\n",
        "      decay_params = [p for n,p in param_dict.items() if p.dim()>=2]\n",
        "      nondecay_params = [p for n,p in param_dict.items() if p.dim()<2]\n",
        "      optim_groups = [\n",
        "          {'params': decay_params, 'weight_decay': weight_decay},\n",
        "          {'params': nondecay_params, 'weight_decay': 0.0}\n",
        "      ]\n",
        "\n",
        "      num_decay_param = sum(p.numel() for p in decay_params)\n",
        "      num_nondecay_param = sum(p.numel() for p in nondecay_params)\n",
        "      print(f'len: {len(decay_params)} num_decay_param: {num_decay_param}')\n",
        "      print(f'len: {len(nondecay_params)} num_nondecay_param: {num_nondecay_param}')\n",
        "      #inspect.signature or __init__.__code__._co_varnames are use to find the constructor , arguments name in the class\n",
        "      #even __init__.__annotations__ this can be used\n",
        "      fused_available = 'fused' in torch.optim.AdamW.__init__.__code__.co_varnames #inspect.signature(torch.optim.AdamW).parameters I didnot use this as this has to use to install external library\n",
        "      used_fused = fused_available and device == 'cuda'\n",
        "      print(f'Using fused in Adam :{used_fused}')\n",
        "      optimizer = torch.optim.AdamW(optim_groups, lr=lr, betas=(0.9, 0.95), eps=1e-5, fused=used_fused)\n",
        "      return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "jiFFKQS_M4j4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL PLANTED 🌱**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "NNZpL-y3Gx6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig(vocab_size=50304))  #making it good number\n",
        "print(model.__dict__)  #good away to know all the layer inside the model"
      ],
      "metadata": {
        "id": "bb72ZAjjNIdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA 📚 LOADING FROM FINEWEB DEVELOPED BY HUGGING FACE 🤗**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "N0_Sb93pEeBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2Bi7v1SqL2mK",
        "outputId": "d0e4cfff-f729-4858-fdcf-e589ff2366f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataload\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "#folder name\n",
        "local_dir = \"filter_fineweb5B\"\n",
        "#size of each file counted as token\n",
        "shard_size = int(1e8)\n",
        "\n",
        "\n",
        "DATA_CACHE_DIR = os.path.join(os.getcwd(), local_dir)\n",
        "os.makedirs(DATA_CACHE_DIR , exist_ok=True)\n",
        "\n",
        "data_files = [f\"sample/10BT/{i:03d}_00000.parquet\" for i in range(2)]\n",
        "\n",
        "# Load only these specific files from the dataset\n",
        "fw = load_dataset(\n",
        "    \"HuggingFaceFW/fineweb-edu\",\n",
        "    data_files=data_files,\n",
        "    split=\"train\"\n",
        ")"
      ],
      "metadata": {
        "id": "oWpGBRtmEcn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(doc):\n",
        "  enc = tiktoken.get_encoding(\"gpt2\")\n",
        "  eot = enc._special_tokens[\"<|endoftext|>\"]\n",
        "  tokens = [eot]\n",
        "  tokens += enc.encode(doc['text'], allowed_special={\"<|endoftext|>\"})\n",
        "  tokens_np = np.array(tokens, dtype=np.uint16)\n",
        "  #ensures that all token values are within the valid range for uint16.\n",
        "  assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"greater than 2**16 ie that is file is too big\"\n",
        "  return tokens_np\n",
        "\n",
        "def save_token_to_file(fileName, tokensNp):\n",
        "  np.save(fileName, tokensNp)\n",
        "\n",
        "#counting num if processor available for parellel computing or tokenizing\n",
        "nprocessors = max(1, int(os.cpu_count()/2) )\n",
        "print(f\"using {nprocessors} processes\")\n",
        "\n",
        "\n",
        "#process shards this code is more cleaner than andrej karpathy\n",
        "def process_shard(dataset, shard_size, DATA_CACHE_DIR, num_processes):\n",
        "  shard_index = 0\n",
        "  all_tokens = []\n",
        "  progress_bar = tqdm(total=shard_size, unit=\"token\", desc=f\"Writing shard {shard_index}\" )\n",
        "\n",
        "  def save_shard(tokens, split):\n",
        "    nonlocal shard_index\n",
        "    file_name = os.path.join(DATA_CACHE_DIR, f\"partialfineweb_{split}_{shard_index:04d}.npy\")\n",
        "    save_token_to_file(file_name, tokens)\n",
        "    shard_index += 1\n",
        "  #using muktiple processor to tokenize fast\n",
        "  with mp.Pool(processes=num_processes) as pool:\n",
        "    for tokens in pool.imap(tokenizer, dataset, chunksize=8):  # here each dataset is using tokensizer as a function a to convert the data\n",
        "        all_tokens.extend(tokens)\n",
        "        progress_bar.update(len(tokens))\n",
        "        # logic for saving shard little improvement then karpathy code\n",
        "        while len(all_tokens) > shard_size:\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            save_shard(all_tokens[:shard_size], split)\n",
        "            all_tokens = all_tokens[shard_size:]\n",
        "\n",
        "\n",
        "            progress_bar.reset(total=shard_size)\n",
        "            progress_bar.set_description(f\"Writing shard {shard_index}\", refresh=False )\n",
        "            # refresh=True then the laoding symbol will referesh again and again one below the other\n",
        "            # for me that is not clean to see that's why I use False it is clean and once .npy file\n",
        "            #is complete then only the next file can be visible\n",
        "\n",
        "    # Save any remaining tokens as the final shard\n",
        "    if all_tokens:\n",
        "        split = \"val\" if shard_index == 0 else \"train\"\n",
        "        save_shard(all_tokens, split)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZeyOw1I5ETwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "#Dataloader for shakespare text\n",
        "class DataLoader:\n",
        "  def __init__(self, B, T, process_rank, num_processes):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    self.process_rank = process_rank\n",
        "    self.num_processes = num_processes\n",
        "\n",
        "    data = open('input.txt','r').read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(data)\n",
        "    self.tokens = torch.tensor(tokens, dtype= torch.long)\n",
        "    print(f'{len(self.tokens)} number of tokens loaded')\n",
        "    print(f'1 epoch is {len(self.tokens)//(self.B*self.T)} loop')\n",
        "\n",
        "    self.counter = self.B*self.T*self.process_rank\n",
        "\n",
        "  def next_batch(self):\n",
        "    b_tokens = self.tokens[self.counter : self.counter + (self.B*self.T)+1]\n",
        "    x = b_tokens[:-1].view(self.B, self.T)\n",
        "    y = b_tokens[1:].view(self.B, self.T)\n",
        "    self.counter += self.B*self.T*self.num_processes #here we advance due to num of GPU\n",
        "    if self.counter + (self.B*self.T*self.num_processes)+1 >= len(self.tokens):\n",
        "      self.counter = self.B*self.T*self.process_rank\n",
        "\n",
        "    return x,y\n",
        "\n",
        "\n",
        "# DataLoader for fineweb\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, batch_size, seq_length, process_rank, num_processes, split):\n",
        "        self.B = batch_size\n",
        "        self.T = seq_length\n",
        "        self.process_rank = process_rank\n",
        "        self.num_processes = num_processes\n",
        "        assert split in {\"train\", \"val\"}, \"Split must be 'train' or 'val'\"\n",
        "\n",
        "        # Get shard filenames for the specified split\n",
        "        self.shards = [\n",
        "            os.path.join(DATA_CACHE_DIR, s)\n",
        "            for s in sorted(os.listdir(DATA_CACHE_DIR))\n",
        "            if split in s\n",
        "        ]\n",
        "        assert len(self.shards) > 0, f\"No shards found for {split} split\"\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_shard = 0\n",
        "        self.tokens = np.load(self.shards[self.current_shard])\n",
        "        self.current_position = self.B * self.T * self.process_rank  # for process_rank 0 current positio will be 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]  # Get tokens for the batch\n",
        "        x = buf[:-1].reshape(B, T)  # Input tokens\n",
        "        y = buf[1:].reshape(B, T)  # Target tokens\n",
        "        self.current_position += B * T * self.num_processes\n",
        "\n",
        "        # If we reach the end of the current shard, move to the next shard\n",
        "        if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):\n",
        "            #we haven't use leftover tokens form the first shards because if we add this to the new added shard then it will currupt the file context meaning\n",
        "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
        "            self.tokens = np.load(self.shards[self.current_shard])  #loads a numpy file\n",
        "            self.current_position = B * T * self.process_rank\n",
        "\n",
        "        x = torch.tensor(x, dtype=torch.long)\n",
        "        y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "YonIbpNxNQRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 524288 #65536 #  #~0.5 M 2**19\n",
        "B = 8\n",
        "T = 1024  #context window\n",
        "assert batch_size % (B*T*ddp_world_size) == 0 , f'not divided check it'\n",
        "grad_accum = batch_size//(B*T*ddp_world_size)\n",
        "if master_process:\n",
        "   print(f' total batch size: {batch_size} |  gradient accumulation: {grad_accum}')\n",
        "print('I am GPU', ddp_local_rank)"
      ],
      "metadata": {
        "id": "FFjKvQAjNTnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = sum(fw['token_count'])\n",
        "print(f'total number of tokens: {num_tokens/1e9:.2f} Billions\\n totals shards:{num_tokens/shard_size} ')"
      ],
      "metadata": {
        "id": "u8qH8y52NCH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#process_shard means it will create a npy file each file will have shard_size{100M} tokens"
      ],
      "metadata": {
        "id": "Sr38-DIOVGjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_shard(fw, shard_size, DATA_CACHE_DIR, nprocessors)"
      ],
      "metadata": {
        "id": "3X-AAhkQXn3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the error I find here:\n",
        "* device mismatch of input and lowerTriangular matrices in casual attention\n",
        "* Error in implementing Custom Linear matrix multipication ka error"
      ],
      "metadata": {
        "id": "Cjubf_r3QEhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoaderLite(batch_size=B,seq_length=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='train')\n",
        "val_loader = DataLoaderLite(batch_size=B, seq_length=T, process_rank=ddp_rank, num_processes=ddp_world_size, split='val')"
      ],
      "metadata": {
        "id": "enju0G6wFH0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HELLOSWAG EVALUATION 🤟**"
      ],
      "metadata": {
        "id": "IdvRycyVOsW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DKo1FnNd6tao",
        "outputId": "2934f9da-30db-48b9-e81b-e5127a9398e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"hellaswag\", split=\"validation\", trust_remote_code=True).select(range(10)) # trust_remote_code=True foe security purpose else it was showing error"
      ],
      "metadata": {
        "id": "paE0ZPBd60Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "code for knowing the details of dataset\n",
        "\n",
        "print(len(dataset))\n",
        "print(\"--- Basic Dataset Information ---\")\n",
        "print(dataset[:2])\n",
        "print(\"\\n\")\n",
        "print(dataset.features)\n",
        "print(dataset.description)\n",
        "print(\"--- First 3 Examples ---\")\n",
        "for i in range(min(2, len(dataset))):\n",
        "    example = dataset[i]\n",
        "    print(f\"Example {i}:\")\n",
        "    for key, value in example.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"-\" * 20)\n",
        "print(\"\\n\")\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "y5v2gXsd7yCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_hellaswag(dataset,model, device, max_seq_length=1024, ddp_world_size,ddp_rank):\n",
        "  \"\"\" this is more readable then andrej karpathy idea or concept I got from andrej the dl god  \"\"\"\n",
        "\n",
        "    # initialize tokenizer\n",
        "\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    pad_token = enc.eot_token  # Using EOS token for padding\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    num_total = 0 #this code is if you are using parrallel gpu then we are tracking example in each gpu else for one gpu you can direcly use len(dataset)\n",
        "    for i,example in enumerate(dataset):\n",
        "\n",
        "\n",
        "        if i% ddp_world_size != ddp_rank:\n",
        "          continue\n",
        "\n",
        "        \"\"\"\n",
        "            This is really important for gpu parrellism\n",
        "            if there are 4 gpu(ddp_world_size)\n",
        "            based on this code the example will distributed in each gpu\n",
        "            for ex:\n",
        "               if i=0 0%4 == 0 not equal 1,2,3\n",
        "               there except ddprank=0 in every other gpu continue will run and skip the rest code\n",
        "               only ddp_rank of 0 i.e. gpu 0 will run the code\n",
        "               in this way you can say for i=1 it will go to gpu 1\n",
        "        \"\"\"\n",
        "        context = example[\"ctx\"]\n",
        "        endings = example[\"endings\"]\n",
        "        label = int(example[\"label\"])\n",
        "\n",
        "        # Tokenize context once\n",
        "        context_tokens = enc.encode(context)\n",
        "        losses = []\n",
        "        max_combined_length = 0\n",
        "        combined_tokens = []\n",
        "        mask_tokens = []\n",
        "        for ending in endings:\n",
        "\n",
        "            ending_tokens = enc.encode(\" \" + ending)  # why use this ' ' because it's obvious after the last word in context there will be gap for sure\n",
        "\n",
        "            # Create combined sequence with padding\n",
        "            combined = context_tokens + ending_tokens\n",
        "\n",
        "            max_combined_length = max(max_combined_length, len(combined))\n",
        "            combined_tokens.append(combined)\n",
        "\n",
        "            mask = [0]*len(context_tokens) + [1]*len(ending_tokens)\n",
        "            mask_tokens.append(mask)\n",
        "\n",
        "        # to handle the limitation of model to take as input very rare but crucial case\n",
        "        if max_combined_length > max_seq_length:\n",
        "            # Truncate from left (beginning) to preserve ending\n",
        "            combined_tokens = [combined[-max_seq_length:] for combined in combined_tokens]\n",
        "            mask_tokens = [mask[-max_seq_length:] for mask in mask_tokens]\n",
        "            max_combined_length = max_seq_length\n",
        "\n",
        "        for token, mask in zip(combined_tokens, mask_tokens):\n",
        "           token += [pad_token] * (max_combined_length - len(token))\n",
        "           mask += [0] * (max_combined_length - len(mask))\n",
        "\n",
        "           input_tensor = torch.tensor(token[:-1], device=device).unsqueeze(0)\n",
        "           target_tensor = torch.tensor(token[1:], device=device).unsqueeze(0)\n",
        "           mask_tensor = torch.tensor(mask[1:], device=device).unsqueeze(0)  #mask should be same as target qki we have to use make in logits that should be same as y in dim\n",
        "           #and for loss of each token we gonna use target only therefore after calculating each lose we gonna remove loss value for ctx_token and padded token\n",
        "\n",
        "           with torch.no_grad():\n",
        "             logits, _ = model(input_tensor) #we are not using target because we gonna use target to calculate loss outside only why losse computation inside model function\n",
        "             loss_per_token = torch.nn.functional.cross_entropy(\n",
        "                 logits.view(-1, logits.shape[-1]),\n",
        "                 target_tensor.view(-1),\n",
        "                 reduction='none'\n",
        "             ).view_as(target_tensor)  # logits.view(-1, logits.shape[-1]) what this is makeing (B*T, C) #y,view(-1) bhi (B*T) we are trying to spread all the tokens in single line so that we can compare logits with target one by one and calculate loss\n",
        "             masked_loss = (loss_per_token*mask_tensor).mean()\n",
        "             losses.append(masked_loss.item() if not torch.isnan(masked_loss) else float('inf'))\n",
        "\n",
        "        if losses:\n",
        "            prediction = np.nanargmin(losses)\n",
        "            correct += int(prediction == label)\n",
        "            num_total += 1\n",
        "\n",
        "    if ddp_world_size > 1 and ddp_rank is not None:\n",
        "        num_total = torch.tensor(num_total, dtype=torch.long, device=device)\n",
        "        correct = torch.tensor(correct, dtype=torch.long, device=device)\n",
        "        dist.all_reduce(correct, op=dist.ReduceOp.SUM)\n",
        "        dist.all_reduce(num_total, op=dist.ReduceOp.SUM)\n",
        "        num_total = num_total.item()\n",
        "        correct = correct.item()\n",
        "\n",
        "    accuracy = correct / num_total if num_total > 0 else 0.0 #to avoid divide by zero\n",
        "    accuracy = round(accuracy, 2)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QTkm8ATws14M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAINING MODEL 💃**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "yBHyHUlNFKAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#converting to tf32 from fp32\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "raw_model = model\n",
        "raw_model.to(device)\n",
        "\n",
        "if ddp:\n",
        "  model = torch.nn.parallel.DistributedDataParallel(raw_model, device_ids=[ddp_local_rank])\n",
        "\n",
        "raw_model = model.module if ddp else raw_model # get unwrapped model if DDP\n",
        "\n",
        "\n",
        "\n",
        "model = torch.compile(raw_model)\n",
        "\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr*0.1\n",
        "warmup_steps = 10  #2000 for llama 2\n",
        "max_steps = 50\n",
        "\n",
        "def get_lr(it):\n",
        "  if it < warmup_steps:\n",
        "    return max_lr*(it+1)/warmup_steps\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "\n",
        "  decay_ratio = (it - warmup_steps)/(max_steps - warmup_steps)\n",
        "  coeff = 0.5*(1.0 + math.cos(math.pi*decay_ratio))\n",
        "  return min_lr + coeff*(max_lr - min_lr)\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "print(f\"Type of raw_model: {type(model)}\")\n",
        "#optimizer\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, lr=6e-4, device=device)\n",
        "\n",
        "\n",
        "#log directory to write helloswag and loss value\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: #open for writing for clearing the file\n",
        "  pass\n",
        "total_steps = 20\n",
        "\n",
        "for i in range(total_steps):\n",
        "      t0 = time.time()\n",
        "\n",
        "      #val loss  same as training below code without validation\n",
        "      if step%10 == 0:\n",
        "        model.eval()\n",
        "        val_loader.reset()\n",
        "        with torch.no_grad():\n",
        "          val_loss_accum2 = 0.0\n",
        "          val_loss_steps = 20\n",
        "          for _ in range(val_loss_steps):\n",
        "            x,y = train_loader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(x, y)\n",
        "            loss = loss/val_loss_steps\n",
        "            val_loss_accum2 += loss.detach()\n",
        "          if ddp:\n",
        "            # Gradient synchronization (all_reduce) happens here for accumulated gradients\n",
        "            torch.distributed.all_reduce(val_loss_accum2, op=torch.distributed.ReduceOp.AVG)\n",
        "          if master_process:\n",
        "            print(f'val loss: {val_loss_accum2.item():.4f} ')\n",
        "            with open(log_file, \"a\") as f:\n",
        "                f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
        "\n",
        "      #sample code once in a while\n",
        "\n",
        "      if step%10 == 0  and False:\n",
        "          model.eval()\n",
        "          num_return_sequences = 3\n",
        "          max_length = 30\n",
        "          enc = tiktoken.get_encoding('gpt2')\n",
        "          tokens = enc.encode('Hello, Why I am thinking')\n",
        "          tokens = torch.tensor(tokens, dtype=torch.long) #(8,)\n",
        "          tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "          x  = tokens.to(device)\n",
        "          sample_rng = torch.Generator(device=device)\n",
        "          sample_rng.manual_seed(42+ddp_rank)\n",
        "          torch.cuda.manual_seed(42)\n",
        "          while x.size(1) < max_length:\n",
        "            with torch.no_grad():\n",
        "              logits, loss = model(x) #(B, T, vocal_size)\n",
        "              #print('logits1', logits1.shape)\n",
        "              logits = logits[:, -1, :]   #(B,vocab_size)\n",
        "              #print('logits1', logits.shape)\n",
        "              probs = F.softmax(logits, dim=-1)\n",
        "              #print('probs', probs.shape)\n",
        "\n",
        "              topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) #we are selecting top 50 along last dim #(B,50)\n",
        "              #print('topk_probs', topk_probs.shape)\n",
        "              next_token = torch.multinomial( topk_probs, num_samples=1,generator=sample_rng,  replacement=True) #(B, 1)\n",
        "              xcol = torch.gather(topk_indices, -1, next_token) #(B, 1)\n",
        "              x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "          for i in range(num_return_sequences):\n",
        "            tokens = x[i,:max_length].tolist()\n",
        "            decode = enc.decode(tokens)\n",
        "            print(f'sequence {i}: {decode}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      loss_accum = 0.0\n",
        "      for j in range(grad_accum):\n",
        "        x,y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        #with torch.autocast(device_type=device, dtype=torch.bfloat16, enabled=False):  #mixed precision where all the other weight are fp32 only the logits are in bfoat16\n",
        "          #in tesla gpu this slow the process TESLA T4 doesn't support bfloat16\n",
        "        logits, loss = model(x, y)\n",
        "          #import code; code.interact(local=locals())\n",
        "        loss = loss/grad_accum\n",
        "        loss_accum += loss.detach()\n",
        "        if ddp:\n",
        "          # Control gradient synchronization for overlap\n",
        "          model.require_backward_grad_sync = (j == grad_accum - 1)\n",
        "        loss.backward()\n",
        "      if ddp:\n",
        "        # Gradient synchronization (all_reduce) happens here for accumulated gradients\n",
        "        torch.distributed.all_reduce(loss_accum, op=torch.distributed.ReduceOp.AVG)  #all the loss in the gpu get accumulated so that we can get the right loss value in the print else it gonna print the loss in the master process only\n",
        "\n",
        "      #gradient clipping\n",
        "      norm = torch.nn.utils.clip_grad_norm_(raw_model.parameters(), 1.0) #this is actually rms of grad if it goes up it is bad for the model\n",
        "      # if the rms of weight exceed 1 then it will clip value\n",
        "      lr = get_lr(i)\n",
        "      for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "      optimizer.step()\n",
        "      torch.cuda.synchronize()\n",
        "      t1 = time.time()\n",
        "      dt = (t1 - t0)*1000\n",
        "      tokens_processed = train_loader.B * train_loader.T*grad_accum*ddp_world_size\n",
        "      tokens_per_second = tokens_processed / (t1 - t0)\n",
        "      if master_process:\n",
        "        print(f' step {i:2d} | loss: {loss_accum.item():.4f} | lr: {lr:.4f} | norm: {norm:.4f} |  dt: {dt:.2f}ms ({tokens_per_second:.2f} tok/s) ')\n",
        "        #saving value of loss function\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"step {i} train {loss_accum.item():.6f}\\n\")\n"
      ],
      "metadata": {
        "id": "vzSmhDgheMGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7Zu56_WNUcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFERENCING CODE 📝**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mPyZQfx1Hzt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 3\n",
        "max_length = 30\n",
        "model.eval()\n",
        "model.to('cuda')\n",
        "\n",
        "\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode('Hello, I am a bad')\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) #(8,)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x  = tokens.to('cuda')\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits, loss = model(x)\n",
        "    #print('logits1', logits1.shape)\n",
        "    logits = logits[:, -1, :]\n",
        "    #print('logits1', logits.shape)\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    #print('probs', probs.shape)\n",
        "\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) #we are selecting top 50 along last dim\n",
        "    #print('topk_probs', topk_probs.shape)\n",
        "    next_token = torch.multinomial( topk_probs, num_samples=1, replacement=True)\n",
        "    xcol = torch.gather(topk_indices, -1, next_token)\n",
        "    x = torch.cat((x, xcol), dim=1)\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i,:max_length].tolist()\n",
        "  decode = enc.decode(tokens)\n",
        "  print(f'sequence {i}: {decode}')"
      ],
      "metadata": {
        "id": "Yu7iuCtcNovv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABYkAAACRCAYAAACG9jd9AAAABHNCSVQICAgIfAhkiAAAABl0RVh0U29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AAAAvdEVYdENyZWF0aW9uIFRpbWUAU2F0dXJkYXkgMDUgSnVseSAyMDI1IDAzOjQxOjQ5IFBN0qquowAAIABJREFUeJzs3XlUFFf6N/Av+yqbsgrRIIKIC4rgAgiCCwb3XTM6GKNRo4kxM8aMjtHEyeIvmsSoMUx8MzozLiGRhMTEuCAiqCgqAkoAg4oBZG1BZF/ePzxdQ9Eo3Q1N0/L9nMM5dHV11VPdVffeeurWLa0ZM2Y0oonExEQQERERERGR/Pz8/JCcnIyysjJ1h0JERESkMG11B0BERERERERERERE6sMkMREREREREREREVEXxiQxERERERERERERURfGJDERERERERERERFRF8YkMREREREREREREVEXxiQxERERERERERERURfGJDERERERERERERFRF6Y1YsSIRnUHQURERERERERERETqwZ7ERERERERERERERF0Yk8REREREREREREREXRiTxERERERERERERERdGJPERERERERERERERF0Yk8REREREREREREREXRiTxERERERERERERERdGJPERERERERERERERF0Yk8REREREREREREREXRiTxERERERERERERERdGJPERERERERERERERF0Yk8REREREREREREREXRiTxKQWTk5OiIiIEP7++9//PnHeN998UzTvvHnzOjBSam/jx49HREQEBg8erPJ1bdiwAfv27VP5enbu3ImtW7eqfD1ttWrVKhw5ckTdYbQqODgYERER8PLyUncoT6UpcZL6aMo+0qNHD0RERGDhwoXqDuWpNCVOQHPqBU2JU1PqL02JU1PKpvawf/9+rF+/Xt1hEBERaQRddQdA8jM2Nsaf//xnDBo0CDo6Orh+/ToOHDiAhw8fqnS9wcHBWL58OQ4ePIjIyEjRe+Hh4aivr8eKFSsUWmZRURG2bdsGAAgNDUXfvn2fOO8PP/yA2NhYmJub45VXXlF8AzqIkZERDhw48MT3X3rpJZX/VtT+XF1dMWrUKJw+fRr37t1TdzhPZGJigtmzZ+PGjRu4fPmyusN5Ik2JEwBmzJiB6upqHDt2TN2htIm9vT3Gjx8Pd3d32NnZQUdHB7m5uThx4gROnz4tM7+8dc2gQYMwatQouLm5wdHRETU1NXjxxRdlljd+/Hj07dsXLi4usLOzw4MHD5CWloaDBw+iqKioTdvm5+cHFxcX/Otf/2rTcpSlKfuIpsSp7t9TXppSL2hKnJpSL2hKnIDmHPNERETUubAnsYYwMjLCO++8g9GjR+PWrVu4efMmfH19sXnzZpiamqo7PIVVVlbi8uXLuHz5MkpLS586761bt3D58mWkpaV1UHTKqaurw7Fjx3Ds2DFcv34dAJCcnCxMq62tVXOEpAwnJyeEhobCzs5O3aE8lZGREUJDQ+Hh4aHuUJ5KU+IEgMDAQPj5+ak7jDYbOHAgXnjhBQDAtWvXcOXKFXTv3h3Lly/H9OnTRfMqUteEhITAz88PEokEDx48eOL6hw8fjiFDhqCoqAhnzpxBWVkZfH198f7778Pa2rpN2+bp6YnQ0NA2LaMtNGUf0ZQ41f17yktT6gVNiVNT6gVNiRPQnGOeiIiIOhf2JNYQAQEBcHZ2FvXmDQ0NRVhYGIKCghAVFaXmCKm2tlbo/RQSEoLBgwfj6tWr7MVBRGqVmZmJVatWobCwUJhmaGiITz/9FDNmzMBPP/0kXMRSpK7573//ix07dqCurg4fffQRjI2NW1z/e++9JzNt1KhReOONNxAaGtrpe40SERERERF1BQonia2trTFnzhx4eHjA0tIS+fn5yMjIwIkTJ3Dr1i3RvD4+Ppg5cybs7Oygra2N7OxsfP311zLzOTk54ZVXXkGvXr1QUFCAkydPwtnZGWPGjMGCBQuEk9edO3eirKwMGzduFD5ra2uLXbt2ISoqCv/+979Fy501axb8/f1hbW2NiooKZGRk4MsvvxT1XF21ahX8/f2xZs0azJs3DwMHDkRtbS1++eUXfP/99zLbb2lpibCwMLi6usLCwgIFBQVIS0tDZGQk8vPzRdu0ePFiPPfcczAxMcH9+/fx/fff4+zZs4p+5QAen7hL45L69ddfMXfuXPj7+3eqJLGJiQkWL14MDw8PWFhY4MGDB7hw4QL+85//oKGhQeXrDwkJQUhICGxsbFBZWYnbt2/jyy+/FCVINMn48ePh4+MDJycnmJmZQSKRIDMzE1999ZVw+3dgYCBeffVVxMTEwMfHBxKJBHv37kVYWBgcHBxw+vRp7N+/X+F1Ozk5iW5Tr6urQ0FBAaKiohAXF9em7TI2NsbKlSvh5eWFhoYGXLt2DeHh4airqxPm8fT0RGBgIJydnWFtbY3y8nLk5uZi//79yMrKEi2vX79+WLJkCezt7ZGTk4Off/65TfGFh4fD0tJSeL1u3Trh/5ycHKxZs0bmM56enpg8eTJcXV1RVFSE/fv3IykpSWY+ecomeb355psYMWKE8Do0NFTUE2/ZsmWQSCSiz9jb28tV3qk7Tj09PYSFhWHIkCGwsLDApUuXEB4eLtMrX966Rh7ShGhTERERwv+7d+9GTEyM2uNUxO3bt2WmVVVVIT09HaNGjcLzzz+PjIwMAIrVNTk5OUrH9PvvvwMArKysFP6sk5MTduzYIZrW9Dc6c+YM9uzZI3q/PesFTdlHlIlT+jl/f384OjoiKysLe/bswf3790XztGcbR5nfUx1xAqqrF9q73aQpcT7L9Ze8car7mFdX/SVv+65Hjx744osvEBUVhZKSklaP+fHjx2PKlCmwsLBARkYGDh8+rFR8REREXZWOo6PjZkU+8Pbbb8PT0xNXr17FL7/8guLiYvTt2xdFRUW4c+eOMF9ISAhWrVoFiUSCH3/8ESkpKXB3d8cLL7yACxcuoLy8HADQrVs3fPjhh+jevTsiIyORlpaGF154AU5OTjAyMsJ3330nNDwnTpyI6upqREdHC+sxNTXFCy+8gPT0dCQnJwvT16xZg4kTJ+LmzZv48ccfUVhYiJEjR2L48OH49ddfhfl8fHzQu3dveHp6Ij09HRcuXICVlRWCg4ORnp4uSvxaWlpi27ZteP7553H27FmcOHEC+fn5cHFxwaNHj4Tt79OnD9577z2YmJjg5MmTiI2NhZWVFaZOnYr8/HzcvXtXka8cALB06VLcuXNHFHtDQwOGDBkCFxcXREZGyjTQV61ahb/+9a+wtbXFpUuXFF6nlLOzM4YNG4bCwkJoaWmhZ8+ewp+Pjw/q6+uF3rKGhob48MMP0a9fP5w/fx4nTpxAbW0tJkyYAEtLS1y5ckVm+SNHjoSDgwOOHj361DjMzMwQEhKCtLQ0pKamtjjP5MmTsXjxYuTk5ODbb79Ffn4+/Pz84Ofnh9OnT4sSkKrk4uKCoUOHIikpCZmZmW1a1muvvYbs7GxcvnwZ58+fh4GBAUaNGgVHR0chUdu7d2/4+PiguroasbGx8Pb2RkBAABITE1FYWIhx48YhPj5e4TGRg4OD4e7ujuTkZERHR+OPP/5A//79MXr0aNy4cUOp8UT79OkDLy8v9O/fHxKJBMePH0d9fT3GjBkDOzs7JCQkCPMuWrQIWlpauHbtGmJiYlBeXo5hw4bBx8cH0dHRwkmMg4MD3nvvPejo6ODbb7/F3bt3MXPmTHTv3h1aWlpKXUTJzs7GxYsXIZFI4OrqioiICBw7dgzx8fG4fPkyCgoKhHknTpwIExMTBAQEIC8vDwUFBejbty98fHyEY0BK3rJJXoWFhbh69SqSkpLg4+ODixcv4uDBg4iPj0d8fDyys7OFssHHxwe9evXC0KFDATweyqVnz57w9vaWKe/UGae0zOnVqxfs7e2Rnp4OU1NTDBw4EFpaWqLjX966Rl5lZWVITU1FfHw83N3dUVJSgr179wpxZmRkoKKiQu1xtoexY8fCxsYGBw8eRE1NDQDl6hoAGDduHMzMzFotx42MjDBs2DC8+OKLsLa2xvHjx4WEsbxqa2uRmZmJ+Ph4dOvWDba2tti2bZvwGyUnJ4uGv2jvekFT9hFF4jQ2NsakSZNgamoKT09PZGRkoLa2Fu7u7ujdu7cosdTebRxFfk91xgmopl5Qpt30rMT5LNZfisTZlesvedt30mPexsYG5ubmOHPmDAoKCuDl5QUXFxfRMR8QEIAVK1YgLy8PR48eRXV1NRYtWgQDAwPk5+e3uXMDERFRV6BQT2IrKyv069cPv/76K7766ithekREBLp16ya8Njc3x/z585GRkYFNmzahvr4eAHDu3Dns3LkTs2bNwq5duwA8vuptZmaG8PBwnDx5EgCQkpKCTz/9VOmN8vLygq+vL3766SdR78mkpCS8++67GDt2LE6dOiVM19LSwrFjx3DixAkAwIULF7Bv3z74+vqKEs+LFi2ChYUFPv74Y1EiKyoqCjY2NsLrxYsXo7a2Fm+//baQRIuJicHf//53zJ49G7GxsQptj4mJCXR0dPDo0SPo6enh//7v/6ClpYW33noL5eXl0NLSgpmZGUpKShT7ohQUFBSEoKAgmelNE4WzZ8+Gg4MDPvroIyQmJgIAzp49C4lEgpkzZ+Knn35Cbm6uSuLT19fHtGnTUFRUhPfee09oYEokEixduhSTJ0/GN998o5J1q9LGjRtFyd2zZ8+ivr5e6E3xxx9/CO/98MMPuHz5Mry9vWFvb499+/bB2toao0ePhru7u8Lf/alTp2QeVpiQkIDt27dj4sSJbRonOj8/X+g9dvbsWZibm2PUqFH49ttvhR6KX3zxhWjb4+LikJubi2XLliEwMFDoLTxt2jQYGBjgo48+QkpKCgDgzp072Lx5M8rKypSKTzqutJmZGYDHvTGf9qAaa2trfPjhh8KJ8rhx47Bs2TL4+PgIJzGKlk3ykPbi6dGjBwCguLj4qXFqa2vj8uXLwkMW7ezs8Nlnn8HPz08o7zpDnADw8OFDbNmyRUji7dq1Cz4+Pjh06BAAxeoaeeXn5wsn8QsXLhTGT+9scbaVq6srBg4ciKtXrwrHmKrrmqa9G8vKyvDvf/9bqHcV0fQ3GT58OAA88TdSRb2gKfuIMnFaWVnh9ddfF3pFrl69GqNHj4alpaUwrb3bOIr8nuqME1BNvaCKdpOmxPms1l/yxNlZjnl11V/ytu+kamtrsWXLFiEZr6Ojg9DQUJibmwu9w6dPn47Kykps3bpVSIbX19dj9uzZCsdHRETUVSn04LrKykrU19fDyclJ5gE2TSt6Nzc3GBsb49y5c0JjAgAePXqE9PR09O/fX5jm7OyM+vp60a1/eXl5CvcsamrgwIEAIHPymZaWhtLSUgwYMEDmM00bURUVFSgrK0P37t1F8/Tr1w+5ubmiBDHwuJeV9HYnXV1duLm5ISUlRaaX5fXr12FnZ6fwA0Sk4zxWV1fD1NQU9vb2sLOzQ7du3VBZWQng8cl9c/n5+bh165ao10JbREdHY9u2baK/5gk4Dw8PFBUVCScQUomJidDR0Wnxu28vvXr1gpmZGRITE0U9N2NiYlBbWwtXV1eVrVuVWur9K+3hYW9vL5ou3ecePXoknChLG89NL+S0Zd05OTnIz89v8wOnLl68KHqdmJgIbW1tuLu7P3X9165dAwDRhRlnZ2dIJBIhQQxA6Z7OyiosLBT1pJKeVDYtR5Qpm1RBekEOAO7fv4+HDx+Kbk3uLHHGxcWJenlmZ2eLhidQpK5hnP9jaGiIFStWoKysDOHh4cJ0Zesaee3evRuffPIJfvnlF1RVVcHAwKBtGyKHzlIvaMo+cvXqVdFt89LettLyVhVtnGc5TnnqBXW2mzQtTkBz6q/W4uwsx7y6yiZ523dSN27cEN3FIh0qSNoONjY2Rs+ePZGUlCQkiKXbR0RERPJTqCdxZWUlvvnmG8yZMwf79u1DTk4Orl+/joSEBPz222/CfNLKfcmSJViyZInMcppW8hYWFnj48KFwq6tUYWEh+vbtq9DGSEmTVzt37mzx/ebJ34aGBpmxxOrq6mBoaCi81tXVRY8ePVq9Iu/o6Ajg8UN5Ro0a1eI8Dg4OMmNoPY00yWdgYACJRIINGzYAePwdGRkZAUCLvSUjIiJEY5G11f3792W2f+nSpaLXVlZWsLS0fOJ6bW1t2y2e5qQ9PJqPMVlTU4MHDx4oNfZlZzBo0CCEhISgd+/esLKygo6OjvBe8ySL9DiqqakR/d/Q0KBUQsbY2BhTp06Fp6cnbG1tRQmi7OxsZTZH0Px3kl7MaPo79ezZE5MnT4abmxtsbW2hp6cnvKevry/8Lx0fvLmioiI4ODi0KU55Nb29HYDoNm4pRcsmVWhsbJS5cFRdXS2UJUDniBOATI/V6upqUbmsSF2jSpoSJ/C4h9vrr78OOzs7fPjhh6K6T9m6Rl7S3o3nz59HXFwc3n33XdTW1rY4Tmd76Sz1gqbsI83bQlVVVQAgdAxQRRtHGZoSpzz1gjrbTVKaEqem1F/yxNlZjnl1lU3ytu+kmieVpRf9pO1S6UWf4uJi0Xy5ubmi5DYRERE9ncIPrjt69CguXryIwMBA2NnZITg4GKGhodizZ49wS1pjYyMA4MiRI62O+SadV1m6urKbUF9fj4aGBmzfvr3F5Ss79qM8DSFpQ0R6EtySlh4i9DQ1NTWorKwUGutNHxJhamqKhoYGpR7EoQqNjY24d++ecJtac6o+IZPG8KxwcXHB+vXrUVZWhqtXryI9PR0VFRVwc3PD1KlToa0t/80AWlpaCq9/+fLlGDlyJJKTk5GYmIi7d++isbERixcvVmp5ijAyMsLbb78Nc3NzJCYm4vjx4ygpKYGBgQFef/11la9fUfKWD6oomxTR2NjYaqydIU6g9e9UkbpGlTQlTm1tbbz55pvw9PTEjh07hKStVEfWNRkZGbh//z6CgoJUmiSWUne9oCn7iDxlA9C+bRxlPCtxAp2j3aQpcWpK/SVPnJpyzKsiTlW0755Wxne29iIREVFnpnCSGHh8VfbgwYMAHvcc2LFjB4KCgoQksXQ80UePHrXa81YikaBXr14wNDQUeoIAaPFW9qqqKtGVZqDlW5Ly8/Ohra2NnJycNj19vam6ujoUFxejZ8+eT52v6RXr1rZdEVlZWXB2doa+vr7QQ1RbWxu9e/dW+snCqlBQUAAbGxuFtr2mpkauZGd1dTUAPHFeaa+N5vuOvr4+LCwscOPGjRY/169fP3Tr1g25ubnttr+0Fz8/P+jp6eGdd94R9UpxdnZW+br19fUxfPhwxMbG4vPPPxem6+rqYvXq1aLjVRnNfydpLyRpj78BAwbA1tZWNF458L9bSZsqLi5usXdQW4fEANo3uaSKskkVNCVOReoaZbTXb6/qOOW1ZMkSDBs2DLt27XpiHB1Z1zQ2NrZp+ArpMp5G2XqhvdYvL03Zl1XVxpHqanECyrWb5KUpcbYn1l+PdeaySZH2nbzy8vIAyPYUd3BwUKhDBRERUVenUK1pYmIi03O3qKgIlZWVorH+bt68CYlEgpCQkBZPAM3NzYX/U1NToaOjA39/f2GanZ0d+vTpI/M5iUQCe3t70e1vgYGBMvNduHABDQ0NmDNnTovb0XRMMEVcunQJDg4OLd66KI2pvr4eiYmJ8PHxQe/evWXma7rtirhw4QKMjIwQGhoqTJs4cSKMjIxw/vz5Fj8ze/ZsfPDBBx36wIaEhARYWVlh4sSJMu8ZGhqKbrWTKiwshK6ubqvDAhQVFaGqquqJtzTevXsXhYWFGDZsmOhiQmBgIPT09JCUlNTi5+bOnYt169bB19f3qetXBx0dHTQ2NooSspaWlggODu6QdQMQHdsAMGPGjBZ/R0WNGDFC9HrYsGGoq6sTfidpWdN0KBpdXV1MnTpVZlk3b96ElZWV6ATDw8OjXW4rlfacVPbYbUpVZRPwuHxsaGiQGS9eGZoSpyJ1jTLKy8vVGueyZcuwfv16DBo0qM0xzJkzB+PGjcO+fftw7ty5J86nTF3zNCYmJnBzc5OZ7u/vD3t7+zb3TJPegvykC0LK1gvyUvc+Iq/2ilNVbRyp1n5Peak6zvasF5RpN8lLU+Jk/dV5j3lVxKlI+05eVVVVyMzMhKenp+g80c/PT+llEhERdUUK9SR+7rnnsHr1aiQkJCA9PR09evTAqFGjYG5uLjrprKmpwaFDh7BixQq8//77iImJQU5ODmxsbDB06FBUVVVh27ZtAIBTp05h2rRpWLRoEUxNTfHo0SOEhoaiqqpKVMkDjxuoQ4cOxd/+9jdkZWXBzc2txXGrsrKycPbsWYwZMwbdunXDpUuXUFxcDBcXF3h7eyMmJgZRUVEKf1lHjx6Fr68vXnvtNbi6uiIzMxO6urrw9fXF+fPnhZ7UR44cweDBg/Huu+/i+PHjuHPnDgwMDODp6Ym+ffti5cqVCq87NjYWQUFBmDNnjvCwPx8fH2RnZwvrbc7W1hYuLi4d2pPil19+QVBQEBYvXoznn38eN27cQG1tLTw8PODt7Y0PPvhA5hbP2NhYzJgxA2+++Sa+//57VFVV4c6dOzJjSAKPH3Y2evRozJ8/H7du3UJ1dbXwpOj6+nr8+OOPeOmll/D3v/8d0dHRsLW1xZQpU1BcXCzzEBMpaQ+Dpg/uUIaenh5efPFFAP8bD3Ho0KHCye7hw4cV7n175coVhISEYNOmTYiOjoa1tTXGjBmDgoKCNp3oyKOyshLp6ekICAhAQ0MDbt++DX9/f/Tq1atdHgjn6OiItWvX4sqVK/Dw8MDQoUOFJ6UDj09MHj58iIULF8LBwQGPHj3ChAkTRA8kkTp+/DjGjRuH1atX4/vvv4e2tjamTZvW4ryKunXrFqqqqjBr1iyYmZnh3r17qKqqEj0kT16qKpuAx/v/77//jpEjR+Lhw4f47bff0NDQgKSkJJlE/7MSpyJ1jTIyMjIwadIkrFmzBomJiaiursbt27cV3v+VjdPDwwMODg64dOmS0tsAAGPGjMHs2bORm5sLBwcHhIWFid6PjY1FVlaW8L+8dc3gwYMxZMgQAI8TLzo6OsKys7KyEBsbCyMjI2zduhXZ2dlIS0uDtrY2XFxc8Pzzz6Ouru6Jt67L6+bNm5g8eTLWrFmDM2fOoLS0FEVFRUI9o2y9IC917yMdHSegmjaOVGu/Z2eJsz3rBWXaTc9anKy/Ou8xr4o4FWnfKeLnn3/G66+/jo0bN+LkyZOwt7dHSEiIwr8NERFRV6ZQklgikSAvLw/9+/dHcHAwGhsb8ccffyA8PFzm5PHMmTN48OAB5s6di0mTJsHMzAwlJSVITk4WJZSrqqqwZcsWLF++HDNmzIC+vj4uXryImpoamV7C0dHReO655+Dn5wcbGxskJSXh+PHj+Oijj2Ri3bNnD+7du4egoCAsWLAARkZGyM7ORnJysugJzoooKyvDunXrEBYWhuHDhyMkJAQ1NTVISEhAWlqaMN+9e/fw1ltv4c9//jMCAgIwdepU1NTUIDU1FUePHlVq3ZWVldiyZQvCwsIwcOBA6OjoIC4uDgcOHMCjR4+UWqYq1NfXY8OGDVi4cCEGDRoEPz8/6OjoICMjA6dOnZJ5kAfweBy7zz//HHPmzMHq1auhpaWFvXv34vTp0zLzHjhwACYmJnjhhRdgaGiIvLw8vPbaa8L7v/zyCxobGxESEoJly5ahsrISv/32G/bu3fvERqKjoyPq6uoQGxvbpm3X1dUV9b4DHj90TtoL8LvvvlM4SZyUlITw8HBMmDAB8+fPR25uLn744Qfk5+djzZo1bYpXHrt378aCBQvg4+OD4cOHIy0tDZs3b8bKlStFD9BTxqFDhzBx4kSMHDkSDx48wJkzZxAeHi68X1paik8++QQzZszAxIkTUVZWhuvXryMyMhJ79uwRLSs/Px/vvfceXn75Zbz44ovQ0dHB6dOnYWNj0+ahOUpLS/H5559j6tSpmD59OgwNDZGTk6P096+KskkqPDwc8+bNg7+/PyZNmgTgcW/U5g95epbilLeuUcZ3330HExMTDBgwACNHjoS2tjZ27979xAtz6oqzNdK7LxwcHFq8YyMrK0tIEitS17i5ucmUedLXcXFxiI2NRVlZGQ4fPgwPDw94eXnBwsICZWVluHLlCr777jtkZma2adsSExNx8OBBjB49Gi+//DJ0dHRw5swZURmhTL0gL03ZR9ozTlW0caTk+T07Q5ztWS8o02561uIEWH915mO+veNUpH2niLi4OBgbG2PKlClYvnw5qqurceDAASxcuFDpZRIREXU1WiNGjOiUT/lauXIlxowZgwULFvAKMKmMq6sr/vGPf+DEiRP45z//qe5wiIiIiIiIiIiIOhxH8qcubdCgQaipqcE333yj7lCIiIiIiIiIiIjUQqHhJoieNd9++y2+/fZbdYdBRERERERERESkNuxJTERERERERERERNSFddoxiYmIiIiIiIiIiIhI9diTmIiIiIiIiIiIiKgLY5KYiIiIiIiIiIiIqAtjkpiIiIiIiIiIiIioC9OoJPHOnTuxdetWdYdBRERERERERERE9MzoNEliV1dXhIWFwcnJSd2hEBEREREREREREXUZnSZJ7OTkhNDQUNjZ2ak7FCIiIiIiIiIiIqIuo9MkiYmIiIiIiIiIiIio4+mqO4Dw8HBYWloKr9etWyf8n5OTgzVr1sh8xtPTE5MnT4arqyuKioqwf/9+JCUlycw3a9Ys+Pv7w9raGhUVFcjIyMCXX36J0tJS1WwMERERERERERERkYbRGjFiRKM6Axg8eDD09fXRv39/TJo0CREREbh9+zYAoKqqCikpKcK8O3fuhK6uLkxNTZGamgoAGDJkCGpqarBixQpUVFQI865Zswa+vr5ISEhAYmIi7O3tMWHCBJSUlGDt2rUdu5FEREREREREREREnZTaexJfv34dAGBmZgYAuH37Ni5fvvzE+a2trfHhhx/iypUrAIBx48Zh2bJl8PHxQUxMDADAy8sLvr6++Omnn7B//37hs0lJSXj33XcxduxYnDp1SkVbRERERERERERERKQ5NG6pFS2RAAAgAElEQVRM4sLCQiFBDAC3bt0CAHTv3l2YNnDgQADAiRMnRJ9NS0tDaWkpBgwY0AGREhEREREREREREXV+au9JrKgHDx6IXkuHmDA2NhamWVtbA3g8PEVLmiaUiYiIiIiIiIiIiLoyjUsSNzQ0tDpPfX09GhoasH37djQ2yg65XF5erorQiIiIiIiIiIiIiDSO0knifv36oVu3bsjNzUVOTk6bA2kpmaus/Px8aGtrIycnR6HYvLy8oK2tjd9++w0PHz5st3iIiIiIiIiIiIiIOiulxySeO3cu1q1bB19f33YJpLS0FABgbm7e5mVduHABDQ0NmDNnTovvW1patjh97dq1WLduHRwdHZ+6/NDQUKxfvx6hoaFtjpWIiIiIiIiIiIhInZTuSayt/Ti/XFdX1y6B3Lp1C1VVVZg1axbMzMxw7949VFVVISUlReFlZWVl4ezZsxgzZgy6deuGS5cuobi4GC4uLvD29kZMTAyioqJkPifdpvr6+qcuv3fv3vDy8mJvYyIiIiIiIiIiItJ4SieJHR0dUVdXh9jY2HYJpLS0FJ9//jmmTp2K6dOnw9DQEDk5OVizZo1Sy9uzZw/u3buHoKAgLFiwAEZGRsjOzkZycjKuXLkiM7+zszN0dXWRk5ODjIyMtm4OERERERERERERkUbQGjFihMKDAbu6uuIf//gHTpw4gX/+85+qiKvDzZgxA/Pnz8enn36K+Ph4dYdDRERERERERERE1CGUGpN40KBBqKmpwTfffNPe8ahN//79cffuXSaIiYiIiIiIiIiIqEtRqicxERERERERERERET0blOpJTERERERERERERETPBiaJiYiIiIiIiIiIiLowJomJiIiIiIiIiIiIujAmiYmIiIiIiIiIiIi6MLUniYODgxEREQEvLy91h6Jy+/fvx/r169UdBhEREREREREREZFA5UniGTNmIDQ0VNWrISIiIiIiIiIiIiIlqDxJHBgYCD8/P1WvhoiIiIiIiIiIiIiUoPbhJoiIiIiIiIiIiIhIfXRVsdDQ0FCEhYWJpkVERAj/7969GzExMaL39fT0EBYWhiFDhsDCwgKXLl1CeHg4amtrRfP5+Phg5syZsLOzg7a2NrKzs/H111/j1q1bSsXq5OSE8ePHw93dHXZ2dqirq0NBQQGioqIQFxcnzNejRw988cUXiIqKQklJCfz9/eHo6IisrCzs2bMH9+/fFy13/PjxmDJlCiwsLJCRkYHDhw8rFR8RERERERERERGRKuk4Ojpubu+FlpWVITU1FfHx8XB3d0dJSQn27t2L+Ph4xMfHIyMjAxUVFQAAZ2dnDBs2DL169YK9vT3S09NhamqKgQMHQktLC6mpqcJyQ0JCsGrVKkgkEvz4449ISUmBu7s7XnjhBVy4cAHl5eUKxxocHAx3d3ckJycjOjoaf/zxB/r374/Ro0fjxo0bKCoqAgAYGxtj0qRJsLGxgbm5Oc6cOYOCggJ4eXnBxcVFlPQOCAjAihUrkJeXh6NHj6K6uhqLFi2CgYEB8vPzRclnIiIiIiIiIiIiInVSSU/i/Px85OfnAwAWLlyIyspKXL58+amfefjwIbZs2YK6ujoAwK5du+Dj44NDhw4BAMzNzTF//nxkZGRg06ZNqK+vBwCcO3cOO3fuxKxZs7Br1y6FYz116hQiIyNF0xISErB9+3ZMnDgRaWlpovdqa2uxZcsWNDQ0AAB0dHQQGhoKc3NzlJaWAgCmT5+OyspKbN26VUiG19fXY/bs2QrHR0RERERERERERKRKnWZM4ri4OCFBDADZ2dmwsrISXru5ucHY2Bjnzp0TEsQA8OjRI6Snp6N///5Krffhw4cy03JycpCfnw9ra2uZ927cuCEkiAGgsLAQAGBvbw/gcY/jnj17IikpSUgQS7ePiIiIiIiIiIiIqLNRSU9iZZSUlIheV1dXw9DQUHhtY2MDAFiyZAmWLFki8/mmiVtFGBsbY+rUqfD09IStrS1MTEyE97Kzs2Xmb55Ulo6ZLP2cnZ0dAKC4uFg0X25urii5TURERERERERERNQZdJokcWtJ3sbGRgDAkSNHcPfu3XZb7/LlyzFy5EgkJycjMTERd+/eRWNjIxYvXgwtLS2FlyeNsyXKLI+IiIiIiIiIiIhIlVSeJH5a0lQROTk5AB4PL9Ha+Mby0tfXx/DhwxEbG4vPP/9cmK6rq4vVq1ejqqpK4WXm5eUBALp37y6a7uDgAG3tTjO6BxERERERERERERGADhiTuLy8HKampm1ezs2bNyGRSBASEiIaEkLK3Nxc4WXq6OgA+N+QEVIzZsyAkZGRUnFWVVUhMzMTnp6eMDY2Fqb7+fkptTwiIiIiIiIiIiIiVVJ5T+KMjAxMmjQJa9asQWJiIqqrq3H79m0UFRUptJyamhocOnQIK1aswPvvv4+YmBjk5OTAxsYGQ4cORVVVFbZt26bQMisrK5Geno6AgAA0NDTg9u3b8Pf3R69evRSOr6mff/4Zr7/+OjZu3IiTJ0/C3t4eISEhMsloIiIiIiIiIiIiInVTeZL4u+++g4mJCQYMGICRI0dCW1sbu3fvRkxMjMLLOnPmDB48eIC5c+di0qRJMDMzQ0lJCZKTk3Hu3Dml4tu9ezcWLFgAHx8fDB8+HGlpadi8eTNWrlwp9DRWVFxcHIyNjTFlyhQsX74c1dXVOHDgABYuXKjU8oiIiIiIiIiIiIhURWvEiBHtM2gwEREREREREREREWkcPkmNiIiIiIiIiIiIqAtjkpiIiIiIiIiIiIioC2OSmIiIiIiIiIiIiKgLY5KYiIiIiIiIiIiIqAtjkpiIiIiIiIiIiIioC2OSmIiIiIiIiIiIiKgLY5KYiIiIiIiIiIiIqAtjkpjUwsnJCREREcLff//73yfO++abb4rmnTdvXgdGSu1t/PjxiIiIwODBg1W+rg0bNmDfvn0qX09L3NzcEBERgenTp6tl/YrYv38/1q9f/8T3p0yZgs8++wxHjhxBREQEgoODFV5HaGio6Dh+55132hKywoKDgxEREQEvL68OXa+iNCXOHj16ICIiAgsXLlR3KK3auXMntm7dqu4wFPLFF1/g3Xff7bD1acrvqWycb7zxBg4dOqSiqKijOTo6YsOGDfjXv/6FiIgI7Ny5U90htYmpqSkmTJgAbe3OcVqmKfVQR5o5cyYiIiLg4uKi0vWsWrUKR44cUek62gP3kc6hveo2Zfe7jm6rqJumtCc1pRyhzktX3QGQfIyNjTFx4kS4u7vD1dUVRkZG+O6773D48GGVrzs4OBjLly/HwYMHERkZKXovPDwc9fX1WLFihULLLCoqwrZt2wA8Th717dv3ifP+8MMPiI2Nhbm5OV555RXFN6CDGBkZ4cCBA098/6WXXsLDhw87MCKiths4cCAWLlyIW7duYc+ePaioqMCdO3cUXk5iYiIKCgoAQCXH8YwZM1BdXY1jx461+7Lbk6bE6efnBxcXF/zrX/9SdyhPpSlx0pPt3r0bNjY2T52ntLQUL7/8cgdFRC0JDAzEq6++Kryur69HSUkJMjMzcfjwYeTl5XVYLEuWLIGrqyuOHz+O3377DdXV1R22blXw9PTEyy+/DIlEgkuXLql8fZpSDz0Lxo8fj6VLl7Y637///W9ERUV1QETy4T7SOhMTE8yePRs3btzA5cuX1R0OtZGrqytGjRqF06dP4969e+oO54m431FHYJJYQ9ja2mLevHnIzc3FrVu3MHDgQHWH1CaVlZVCwebn5/fUeW/dugUA6Nmzp8rjaou6ujqhMeXo6IjBgwcjOTlZqGhqa2vVGR6RUtzd3QEAu3btQk5OjtLLyc/PR35+PoDHF0zaW2BgIB49etTpT2g0JU5PT08EBAR0+uSrpsSpKEUvvGqy6OhodOvWTXg9ZswYaGtr4/Tp08K0ysrKNq3jk08+adPn6X9SU1Nx9+5d6OnpwcXFBSNHjsSAAQPwl7/8BRKJpENi6Nu3LxITE/Hvf/+7Q9anaufPn8eiRYsQEBDwxCRx3759kZ+fj7KysjavT1PqoWfB3bt3Rd9zz5494enpiZSUFGRnZwvTs7Ky1BHeE3EfaZ2RkRFCQ0MBQG3JOnXXbc9SW8XJyQmhoaG4ceNGp04Sd4b9jp59TBJriPz8fKxcuRKFhYUYOXKkxieJn0W1tbVCoiIkJASDBw/G1atX2cAijWZmZgYA7XJiSkTU3HfffSd67e3tDQMDg2cu8f+sSE5OFt1VNmXKFCxcuBBTpkzB/v37Vb5+Q0NDGBgYoKKiQuXr6igNDQ04deoUpk2bBisrK5SUlAjv9ezZE3PnzoWPjw+OHTv2zCTGu4r09HSkp6cLr8ePHw9PT08kJSV1qp7DREREUgonia2trTFnzhx4eHjA0tIS+fn5yMjIwIkTJ4Qen1I+Pj6YOXMm7OzsoK2tjezsbHz99dcy8zk5OeGVV15Br169UFBQgJMnT8LZ2RljxozBggULhB6YO3fuRFlZGTZu3Ch81tbWFrt27UJUVJRMw2nWrFnw9/eHtbU1KioqkJGRgS+//BKlpaXCPKtWrYK/vz/WrFmDefPmYeDAgaitrcUvv/yC77//Xmb7LS0tERYWBldXV1hYWKCgoABpaWmIjIwUeslJt2nx4sV47rnnYGJigvv37+P777/H2bNnFf3KAQAVFRUa0yA2MTHB4sWL4eHhAQsLCzx48AAXLlzAf/7zHzQ0NKh8/SEhIQgJCYGNjQ0qKytx+/ZtfPnllygsLFT5ulVh/Pjx8PHxgZOTE8zMzCCRSJCZmYmvvvpKGL5CehtoTEwMfHx8IJFIsHfvXoSFhcHBwQGnT59W6uTNyckJ48ePh7u7O+zs7FBXV4eCggJERUUhLi6uTdtlbGyMlStXwsvLCw0NDbh27RrCw8NRV1cnzOPp6YnAwEA4OzvD2toa5eXlyM3Nxf79+2V6XfTr1w9LliyBvb09cnJy8PPPP7cpPkXo6OjgpZdegre3N/T19XH16lWcO3fuifOvX78e7u7u2LBhA2bMmIF+/frB2toaubm52Lx5s9AbS95yRJ59pOm8U6ZMgYWFBTIyMlocsqZHjx744osvRNP+3//7f8L/e/fuFXr5ScvQuXPniubfvXs3JBKJqLxub6GhoQgLCxNNi4iIEMUQExMjel9PTw9hYWEYMmQILCwscOnSJYSHh8v09Je3/lJ3nO1Z1zg5OWHHjh1PjPPMmTPYs2dPi9vn7+8PR0dHZGVlYc+ePbh//36ni9PT0xOTJ0+Gq6srioqKsH//fiQlJYnmae/6y8TEBJMmTcLAgQPh4OAAfX19FBcX49KlSzJj8Te/nT8tLQ2bNm1qcbmKtMV0dHQwd+5cjBgxAt27d0dFRQUyMzNx7Ngx3LhxQ2bZ8vyenUFrcVpaWiI8PFx4XVdXh/nz57e4LB0dHcyZMwfe3t6ws7NDaWkpfv/9d1y4cAHx8fFKxxgYGIhx48bByckJtbW1+OOPP5CUlCQzdJe87WBAsTbOvn37kJWVhePHj+OFF15A3759YWRkhISEBHz88cdKb1dTCQkJWLhwIZ577jmZbZL3mJenTly1ahUCAgKEz4wdOxZjx44FAOTl5eG1114T3tPV1cWSJUvg6ekpHMfXr1/Hvn37ZMpQFxcXfPDBBzh8+DCqq6sxcuRIODs7Q0tLC0eOHEFkZCQ+/fRTPHz4EIaGhrC1tcWJEyfw6NEjTJs2DQ8ePMDHH3/c5p5nv/76K6ZNm4Y5c+Zg7969sLa2xp/+9CeMGDEC5eXlOHToEH744Qell68p9aUiFClfFTnfs7S0xKuvvgoXFxdUVFTg3LlzHX43oL29vVznpfKc68qrq+8jgHxl9ptvvokRI0YIr0NDQ4WenQCwbNky0V0VAwYMwJ/+9Cc4ODhAS0sLubm5+M9//oOUlBRhHmm7OyoqCiUlJWqr2+TZ7+Rtq0iHq9y+fTv69ev31H1ER0cHS5YswbBhw6Cjo4Nr164hLi4OGzZsaHG4y/YUHh4OS0tL4fW6deuE/3NycrBmzRqZz8jTngTa9/hUdL8D1FOO0LNB4STxa6+9BhcXF8THxyM1NRXW1tYYPHgwHB0dRQV9SEgIlixZgqysLGFA9QkTJuCdd97BX//6V6Gw69atGzZv3gx9fX1ERkaivLwcoaGhMDIyatOGrVmzBr6+vkhISEBkZCTs7e2F9a9du1Y0r5aWFtavX4+EhAQkJyfD398fL774IrKyspCcnCzMZ2lpiW3btsHU1BS//vorfv/9d3Tv3h2enp5wd3cXksR9+vTB5s2bUVVVhRMnTqCgoAAjR47EqlWr0NjYiNjY2DZtm7ykDeqzZ89i165dbV6enZ0dvL29RdN0dHRQX18vvDY0NMT7778Pa2trREdHIzMzE25ubggNDYWhoaGoUlOFyZMnY9GiRUhLS8P3338PW1tbTJkyBVu3bsWaNWvafMuqOoSGhiItLQ3Xrl1DeXk5hg4dilGjRsHAwAAffvihaF57e3v8+OOPmDlzJjZt2oTTp08jJycHkyZNwsmTJ5Gbm6vQuocNG4bevXvjypUruHfvHmxsbDB+/HisWrUKxcXFSEtLU3q7lixZgps3b+LAgQPw8PDAmDFjoKenh88++0yYZ/z48WhoaEBMTAwKCgrQp08fjBs3Dm+99RbeeOMN4cKJg4MDNm7ciIqKCqG8mT9/PnR1O+ZmiWXLliEoKAjx8fFISkrC4MGD5boF669//Svy8vJw9OhRGBgYwMvLC6amppBIJAqVI/LuIwEBAVi6dCkyMzMRGRkJGxsbvPXWW9DT0xPFVVZWJowZHhQUhGHDhuHTTz9FTU0NACg1JrEqNB3n+OWXX0ZlZaWo0X/79m2Zz8ybNw9aWlpIT0+Hm5sbAgMDUVJSInrwh7z1l7rjbO+6pulY8SEhIRg0aJDwWvp+c15eXujWrRuuXbuG2tpauLu7Y8WKFaIHE3aGOK2srLB27VqkpqYiJSUFQ4YMwRtvvIEVK1YI5Ygq6i97e3v4+fkhMTERZ8+ehZaWFgICAjBt2jRUVFSITnxSU1OF7Wht3G5522IAsGnTJvTv3x/nz59HZGQkDAwM0LdvX4wePVomSSzP79kZyBNneXm56NkHbm5uT1zevHnzMG3aNCQnJ+PXX3+Frq4u3NzcMGDAAKWTxEOHDsWrr76Ke/fuISoqCsXFxXBxccGwYcNEv7si7WBl2jgWFhZ48803ER0djdjYWFhbW6N///5KbVNLpONJl5eXC9OUPeafVif+/PPPSEhIgJ6eHt544w1cuXJFuFjZfEzitWvXwtvbGzExMUhNTYWHhweCg4Nhbm6Ojz76qMV1Dxs2DLa2tjh+/DiOHz+OPn36oEePHsL7rq6uOHz4MLy9vTF16lTk5ubi8OHDmDlzJmbNmtXm275LS0sRHx+PMWPGoKysDCEhIQCAo0ePIjIyUqh/laUp9aUiFClfAfnO97S1tbFp0ybY29vjhx9+QEFBAYKCgmBtba2y7WjJ3/72N0gkEly9ehVDhgxp8bxUkXNdeXT1fUTeMlv6rBwTExO8+uqruHjxoqhMa1oWurm5YePGjSgpKcGRI0dQV1eHyZMnY8OGDdi8eTN+++03Ubz+/v7Iy8vDyZMn4eDggHHjxnVo3SbPfqdIWwUAwsLCkJqaisjISHh4eLS4j0jPoxISEnD58mUMHDgQK1eubHXZ7WH37t3Q19dH//79MWnSJERERAj7elVVlcz88rQngfY/PhXZ76TUUY7Qs0GhDIqVlRX69euHX3/9FV999ZUwPSIiQjSenLm5OebPn4+MjAxs2rRJSCKeO3cOO3fuxKxZs4SkZWhoKMzMzBAeHo6TJ08CAFJSUvDpp58qvVFeXl7w9fXFTz/9JOo9mZSUhHfffRdjx47FqVOnhOlaWlo4duwYTpw4AQC4cOEC9u3bB19fX9FBtGjRIlhYWODjjz9GQkKCMD0qKkr00JXFixejtrYWb7/9tnCyGhMTg7///e+YPXt2hyWJ21tQUBCCgoJkpjc9IZ89ezYcHBzw0UcfITExEQBw9uxZSCQSzJw5Ez/99JPCiUp56evrY9q0aSgqKsJ7770nXKGUSCRYunQpJk+ejG+++UYl61aljRs3inqDnj17FvX19cJV5j/++EN474cffsDly5fh7e0Ne3t77Nu3D9bW1hg9ejTc3d0V/u5PnTol08hOSEjA9u3bMXHixDYlifPz84XegGfPnoW5uTlGjRqFb7/9Vhj79osvvhBte1xcHHJzc7Fs2TIEBgYKvYWnTZsGAwMDfPTRR8KV+Tt37mDz5s0qHybB2toaAQEBuHHjhlBuxcTEYNOmTaIr080ZGxsjKSlJdGLZdGgSRcoRefeR6dOno7KyElu3bhUaMvX19Zg9e7YotpqaGmGcK09PTwCPbzHubA9ebDrO8cKFC0VjnT/Jw4cPsWXLFqHH+q5du+Dj4yM0VhWpv9QZJ9D+dU3TuIYPHw6g9fHOrKys8Prrrwu9F1avXo3Ro0fD0tJSmNYZ4rS2tsaHH36IK1euAADGjRuHZcuWwcfHR+gZpYr6Kzc3F2+88YboDono6Gjs2LEDEyZMEJWvRUVFwvfztHG75W2LAY8vtPXv3x8///wzvv76a2H68ePHYW9v3+KyW/s9OwN54qytrZX72Qc+Pj64c+cO3nvvPWHasWPHYGpqqnSMI0eORENDA9555x2h7Dxz5ozMMuVtByvbxunduzf27duH48ePK70tLTE2NoaPjw8WLFgAAKJ2sTLHfGt1YlZWFrKysoTkuUQiafG479evH7y9vZGQkIDdu3cDeHwc6+vrw9fXF+7u7i22XZydnbFu3TrcvXsXAGTuBvr9998RGRmJ+/fvY+3atYiLi8Mvv/wCFxeXpz58WRGxsbEIDAzE9OnTkZKSgi+//FJ0l2JbaEp9qQhFyldAvvO90aNHw9HRUfRw8PPnz7d4d4qqaGtr4/Lly8LDsO3s7PDZZ5/Bz89PiFPRc115dPV9RN4yW3ohVnoRqbi4+Inf0+zZs6Gjo4Nt27YJHSxSUlLwySefYPbs2aI6B3hcb23ZskW4c0lHRwehoaEwNzcXenWqqm6TZ78D5G+rSP3222/CbxwTEwMnJyfRPiI9j8rIyBDucDl79iw2btz41POo9nL9+nUA/xte7/bt20/d7+VpT6ri+FRkvwPUV47Qs0FbkZkrKytRX18PJycnmcKlafLAzc0NxsbGOHfunKiX6aNHj5Ceni7qweDs7Iz6+nrR7Wd5eXn4/fffFd4YKel4vdJGgFRaWhpKS0sxYMAAmc80PcgqKipQVlaG7t27i+bp168fcnNzRQ1h4PFYYtKroNIrdCkpKTK9ma5fvw47OzvY2dkpvW2KyM/Px61bt9qtgRkdHY1t27aJ/pon4Dw8PFBUVCScYEslJiZCR0enxe++vfTq1QtmZmZITEwU3cISExOD2tpauLq6qmzdqtRSYi41NRUAZE7ypfvco0ePhBNlaaOiefJA2XXn5OQgPz+/zb0qLl68KHqdmJgIbW1t4UFpT1r/tWvXAEB0YcbZ2RkSiUR069aNGzda7FHY3lxdXaGjoyNTUcvzhPIn3TqqaDkizz5ibGyMnj17IikpSXSlu63DhmiauLg40clCdnY2rKyshNeK1F/qjLOz1DVXr14VJQ+lCRbp8dlZ4iwsLBQa9MD/GttN63lV1F8VFRWi3xF4fGtoeno6rKysoK2tUDMMgPxtMeDxNgFo8fbCvLw8mWmt/Z6dRXvHWVlZCSsrKzg4OIimt9QzR14VFRXQ1taW2W+aL1PedrCybZzS0lIh+dweFixYgIiICOzfvx+vvvoqzM3NcezYMZw/fx5A2475tgynICVtQzTvJSdN+kqPieauXr0q7EctefDgAYD/jc8v3baKigqYmJi0LWgAQ4YMwbp161BUVIQdO3bg3Xffbbf2u7I6e32pTPna2vmeNOHfdP+prKwUJck6QtNj9v79+3j48KEoYabMua4qPEv7iLxltiKcnZ2RlZUlugMvNzcXmZmZLV5cunHjhmhoK+kwQi1d1JWHonVba/udMpoPw1BQUCDaR6TnURcuXBDN11kfyiZPe7KzHJ+aUo5Q56NQT+LKykp88803mDNnDvbt24ecnBxcv34dCQkJotslpI30JUuWYMmSJTLLaVr4WVhY4OHDhzK3URUWFip9ZV6avNq5c2eL7zdP/jY0NMj0kKmrq4OhoaHwWldXFz169Gi1wHJ0dAQAjBo1CqNGjWpxHgcHhw4Z3y8iIkI0llRb3b9/X2b7ly5dKnptZWUFS0vLJ67X1ta23eJpTnplrfm4fDU1NXjw4IGoQtIkgwYNQkhICHr37g0rKyvo6OgI7xkYGIjmlR5HNTU1ov8bGhpk5pWHsbExpk6dCk9PT9ja2opOhJo+lVkZzX8n6clQ09+pZ8+emDx5Mtzc3GBraysaFkFfX1/4Xzo+eHNFRUUyDaP2Ji1Pmq+/tWO8oaHhicM2KFqOyLOPSE/Ki4uLRcvJzc0VNdyfdU0fCAQ8vk25aVmvSP2lSq3F2VnqmuZ1p/TWPGnysrPEKU3wSEkvlBgbGwvTVFV/jRs3Dn5+frC3t4eFhQW0tLSE94yMjPDo0SOFlidvW0wa84MHD+TuBdza79lZtHec33zzDVatWoXPPvsMRUVFuHHjBq5cuSJz0qqIn376CYMGDcLatWtRXl6O9PR0Ybz6psNCyNsOVraNk5eX165lfHR0tOhCyv3790Xj8Sp7zD+tTlSE9HtovnzpHUrNzwGkWrtLQJqYlw5t0bSNpUz7qqkePXpg1apVyM7OxtatWzvN0GiaUF8qUr7Kc75nZWWFxsZGYX+RaqmNqSqNjY0yFwiqq6tFQ9Aoeq6rKs/SPiJvmS0vPT09mJqaytTLwOPzEzc3N5iYmIj20eYXeqXljiTgo0AAACAASURBVLIXohSp2+TZ75TRfJvq6+tF+4h0X22+bnVfJHsSedqTneH41KRyhDofhQfsPHr0KC5evIjAwEDY2dkhODgYoaGh2LNnj9DFvrGxEQBw5MiRp16Vbzqvsloac7S+vh4NDQ3Yvn17i8tX9oqgPBWZtCF+/vz5J/bQa2lMp2dFY2Mj7t27J7oluqmOSI63dZ/qTFxcXLB+/XqUlZXh6tWrSE9PR0VFBdzc3DB16lSFeqE1bRTJa/ny5Rg5ciSSk5ORmJiIu3fvorGxEYsXL1ZqeYowMjLC22+/DXNzcyQmJuL48eMoKSmBgYEBXn/9dZWvX9WqqqqeWKYoUo7Iu4887bhQxXepTA/JjtBaOa5I/aVKrcXZWeqaZyVOQDX118SJE/HSSy/hzp07iI+PR2ZmJmprazF27FgMHTpU6eNEnraYdJsU0VEXQdqqveO8evUqXnvtNQQFBcHR0REDBw5EQEAAoqOjZR7iKa/CwkL85S9/gb+/P/r06YN+/fph6dKlmDBhAt566y2hd5uiv5Gi87c0pmJbtNRhoCllj/mn1YnKUPR7asvDodtah86cORNGRkb45JNPOk2CGOj89aUqytfOcA7R2NgoV92qinNdRT1L+4i8ZXZ7UvX+pkjdJs9+R/Lng9R9fGpSOUKdj1JPdcrNzcXBgwcBPO6lsmPHDgQFBQknJtKrr48ePWq1561EIkGvXr1gaGgoasi2dCt7VVWVzAOWWrq1MD8/H9ra2sjJyZG5Eqysuro6FBcXo2fPnk+dr2mvvM56m4QqFRQUwMbGRqFtr6mpkashJ+298aR5pVfLmu87+vr6sLCwaPEp7sDjYUS6deuG3Nzcdttf2oufnx/09PTwzjvviK4GOjs7q3zd+vr6GD58OGJjY/H5558L03V1dbF69eo2n3g2/52kvfSkvaQGDBgAW1tb0TiNwP9ujWmquLi4xaudHfGgEWm8zcuittxCr0g5Iu8+Ir21vPn35ODg0KaEbmVlJbS1tUVluK6uLiwsLGR6LUvV1NSIeju3h/ZqaCtSfymjveJUdV3T1eIElKu/WjN69Gjcv38fb731lqixPnHixDYvu7W2GPC4XnRxcel0Ywp3RuXl5YiKihJeb9++HaNHj1Y6SQw8PgGLiYkRfpNXXnkFY8eOhYeHhzAOorztYGXbOB1N3e1g6TAQdnZ2op7J0vZ7RwxDpaghQ4YgISGhw2JTd33ZXu1uVZSvxcXF0NLSQs+ePUU95DvbkDuqONdtSt37SHtRdB+Rp8yWV21tLR4+fNjiXR49evRARUVFmy5OyUsVdVt7etJ5VGt3b7X3+Xt7tidVfXy2F02JkzqeQpkBExMTmZ67RUVFqKysFI2PdvPmTUgkEoSEhLR4e4S5ubnwf2pqKnR0dODv7y9Ms7OzQ58+fWQ+J5FIYG9vL+rOHxgYKDPfhQsX0NDQgDlz5rS4HcqOrXPp0iU4ODi0ePucNKb6+nokJibCx8cHvXv3lpmv6bar2uzZs/HBBx/IPJRKlRISEmBlZdVi5WtoaNjiLSuFhYXQ1dVtdViAoqIiVFVVPbHSuHv3LgoLCzFs2DDRxYTAwEDo6enJjIkkNXfuXKxbtw6+vr5PXb866OjooLGxUXTiaGlpieDg4A5ZNwDRsQ0AM2bMaPOtRwAwYsQI0ethw4ahrq5O+J2kZU3TW3B1dXUxdepUmWXdvHkTVlZWogSyh4fHU2+TGTRoENavX49ly5a1aTtSU1NRXV0Nb29v0XQfHx+ll6lIOSLvPlJVVYXMzEx4enqKytDWHnrRGmniqenTlceOHdviXR5SxcXF6NGjR7v2Ni4vL2+XW+IVqb+U0V5xqrqukd4e2NYLLZoSJ6Bc/dUabW1t1NXViU5OBw0a1OLFLnnJ2xYDIIwTO2PGDJnlNC0HlGFmZob169dj/fr1wgNfNFVL8RcUFMh8n21dprQ3etN6Td52sLJtnI6m7nbw1atX0dDQINNW9/f3R2Njo2gsSVVS5PgwNTXt0B7E6q4v26vdrYryVTr2cNPYjIyMMHjwYOUDVQFVnetKqXsfWbZsGdavX49Bgwa1af2K7CPyltlSEokEDQ0NT/2eUlNT0adPH/Tq1UuY5uDggL59+3bIONeqqNvaW0pKCqqrq2XOC5ufVzXX3ufv0mf4tEcdpcrjU579Tl6qLkdIcynUk/i5557D6tWrkZCQgPT0dPTo0QOjRo2Cubm56CnANTU1OHToEFasWIH3338fMTExyMnJgY2NDYYOHYqqqips27YNAHDq1ClMmzYNixYtwv9v776jojrTP4B/qYIoCCs1IHbAQXrviiIuoEisuEasURTFEjXRWFcT3Y0mcXUNSoq7P41BBY2FKNKsKCogqCACEQFBmtSR+vuDM3e5DMrMwDiMPp9zPMcZ7tz73Jl73/6+t1+/fqitrYWPjw+4XC5fBSYpKQlWVlb44osvkJOTAyMjI9a6pDw5OTlISEjAmDFj0L9/f9y+fRtlZWUYPnw4bG1tER8fz+pRE9Tp06fh7OyMFStWYOTIkXjy5Ank5eXh7OyMGzduML2OJ06cgLm5ObZv347o6Gjk5eWhT58+sLCwwIgRIxAcHCz0sYG2h4UoKioylWEOh4OgoCAAwKlTp/jW/NHW1sbw4cPfac/QxYsXMXbsWMybNw9DhgxBRkYGGhsbweFwYGtri6+++opvmmFiYiICAgKwZs0aREVFgcvlIi8vj2/dPaDtYWdubm6YNWsWsrOz8fr1ayaTbW5uxu+//4758+fjyy+/RGxsLLS1tTFp0iSUlZXxLcrOw2uo6u40IgUFBcyePRvA/9bks7KyYn6vX3/9VejRt3fv3oW3tzc2b96M2NhYaGpqYsyYMSgpKRF7wl1fX4/MzEy4u7ujpaUFubm5cHV1haGhYY+MdtHX18fq1atx9+5dcDgcWFlZISEhgWl0fPjwIaqrqzFnzhzo6emhtrYWEyZM6LTXPTo6GuPHj0dISAiioqIgKysLf3//t/bQDxw4ENbW1l2uQ9iV6upqJCYmYvz48QgNDUVKSgosLCw67egShqDpiDDXyIULF7By5Ups2rQJly9fhq6uLry9vbtVWLx+/TpmzJiB4OBg3Lx5E/r6+hgyZAjfQy3bu3r1KoKDg7F27VokJCSgpaUFqampnRbCBZWVlQVfX1+EhoYiOTkZr1+/Rm5urtDXqjD5lyTjBMSX1wBt95+fnx9CQ0MRFxeHV69eobS0VKSlIaQlTlHyr66kpqZiypQp2LBhA+7cuQMzMzM4ODggLy+PrwGtfYVIQUEBKioqzHvt8zpBy2JAW+d2WloavL29oaqqipSUFHC5XFhaWqKpqQlhYWFCf088ioqKsLa2Zv7f25iYmDCVKDU1NcjIyLC+4/Zpzu7du5Geno60tDTIycnB0tISNjY2iIuLE/n4ISEhaGpqwv3791FTU4NRo0bBzc0Nz58/x5MnT5jtBC0Hi1rGkQRx3vNdyc/Px61bt+Dk5IRly5YhPT0dHA4Hzs7OuH37do+seywIYe6PgoIC2NjY4Ny5c90ukwhC0vllT5W7hUlfBXXnzh3k5+fD398fQFuD2rhx43rdEmfiquvySPoa4XA40NPTE+gh0G8jzDUiaJrN09zcjKdPn8LR0RHV1dV4/PgxWlpakJKSwpSrz5w5A3t7e6xfvx7nz59HU1MT/Pz80NraKvLvI+m8TdCyiqBqamqQkJAALy8vfPbZZ7h9+zZMTU27fDZVT6UjPNnZ2eByuZg6dSpUVVWRn58PLpfLeii6oMR5fwpy3fWGOIl0E6qRuKKiAkVFRRg1ahQ8PT3R2tqK58+fIywsjDW9EQDi4uJQWVmJGTNmwNfXF6qqqigvL0daWhqrEsPlcrFt2zYsWbIEAQEBUFRUxK1bt9DQ0MA3Sjg2NhaDBg2Ci4sLtLS0kJKSgujoaOzevZsv1oMHDyI/Px9jx45FYGAglJWV8ezZM6SlpYk8iqCqqgrr1q1DUFAQ7O3t4e3tjYaGBiQlJeHRo0fMdvn5+Vi/fj3mzp0Ld3d3TJ48GQ0NDUhPT8fp06dFOjYAeHt7s0YyGRsbw9jYGEBb5bZjI7EkNDc3Y+PGjZgzZw7MzMzg4uICOTk5ZGVlISYmptNF6F+8eIH9+/dj+vTpCAkJgYyMDA4dOoQrV67wbXv06FGoqKjgr3/9K5SUlFBUVIQVK1Ywf7948SJaW1vh7e2NxYsXo76+Ho8fP8ahQ4femHDq6+ujqakJiYmJ3Tp3eXl5+Pj4sN4zMzNjesFPnToldCNxSkoKwsLCMGHCBMyaNQuFhYU4c+YMiouLERoa2q14BXHgwAEEBgbCzs4O9vb2ePToEbZu3Yrg4OBuLxdw/PhxTJw4EY6OjqisrERcXByrweLVq1fYt28fAgICMHHiRFRVVSE1NRWRkZE4ePAga1/FxcXYsWMHFi5ciNmzZ0NOTg5XrlyBlpbWO1maIzw8HK2trbC1tYWzszOKi4tx9OhRLFmyROR9CpqOCHONXLt2DX379sWkSZOwZMkSvH79GkePHsWcOXNEjrO4uBgHDx7E1KlT4enpiYyMDOzcuRNr1qx542fi4uKgp6cHd3d3pmC5dOnSbnU+nDp1CioqKjA1NYWjoyNkZWVx4MABvrxJEILmX5KOU1x5DQAkJyfj2LFjcHNzw8KFCyEnJ4e4uDi+e+99ilOU/KsrJ0+ehJycHOzt7cHhcJCdnY1vv/0WpqamfBXUtWvXskbXq6mpYd26dQDAyuuEKYsBwK5duzBjxgw4ODjAxsYGcnJySElJwfnz54U+H2kye/Zs1gwHAMz3CbDTnMePH8PAwADm5uZQUVFBcXExIiMju3V9Pnz4ELa2tvD19YWmpibKy8uRlJSEyMhIVqVWmHKwKGUcSRDnPS+I/fv3o7a2FpaWlnBxcUFlZSViYmLw448/iv3YoggPD8dnn32Gf/zjH0hOTuZbGkZWVhZaWlrQ19fH4cOHhZ723pGk88ueKncLk74KY8eOHVi2bBkz+jUjIwN//PEHpkyZ0q14e5o46ro8kr5Geoow14igaXZ7YWFhmDlzJlxdXeHr6wugbRQ07x7OycnBtm3bMHfuXMyYMQMyMjIoLCzEDz/80GnDsyAknbcJWlYRxo8//ggZGRnY2Nhg1KhRSEtLw08//YRly5aJvf7O8+rVK+zfvx+TJ0/GlClToKSkhIKCApHr2+K8P7u67npLnER6yTg4OEh+hf5OBAcHY8yYMQgMDOxVBV/yfhk5ciR27tyJS5cu4fDhw5IOhxBCCCGEysEfmP79+yMgIACmpqYYNGgQa5RcYWEhGhoaUFVVhaNHj0r12pFU7iaECMLJyQmrVq3Cvn37mKWzeCgdIUS8RHpwHSHvCzMzMzQ0NOC3336TdCiEEEIIIeQDVF1djV9++UXSYYgdlbsJIR2pqqryLVPH4XDQ3NyMnJwcvu0pHSFEvKiRmHzQTp48iZMnT0o6DEIIIYQQQt5rVO4mhHTk6uoKZ2dnXLt2Da9evYK5uTnc3d0RHR3NPDywPUpHCBEvaiQmhBBCCCGEEEIIIe9Ufn4+lJWVMW/ePDQ3N6OsrAxRUVE4fvy4pEMj5IPUa9ckJoQQQgghhBBCCCGEECJ+sl1vQgghhBBCCCGEEEIIIeR9RY3EhBBCCCGEEEIIIYQQ8gGjRmJCCCGEEEIIIYQQQgj5gFEjMZEIAwMDREREMP/+7//+743brlmzhrXtzJkz32GkpKd5eXkhIiIC5ubmYj/Wxo0bER4eLvbjCGrgwIGIiIjAnDlzJB3KW4ka56pVq97pQyaESUfEwdPTExEREbC2tn6nxxXW+37dScL333+Pv//975IOo0vSEufy5ctx4sQJSYfRJWm556Ulzt50z7/r/IsQ8u5JS14jLXFKS14jLXFKmrW1NSIiIjBp0iRJh0IkTF7SARDBeHh4wM7ODoaGhtDQ0EBlZSUyMzNx/PhxFBcXi/XYnp6eWLJkCY4dO4bIyEjW38LCwtDc3IylS5cKtc/S0lLs2bMHAODj44MRI0a8cdszZ84gMTERampq+PTTT4U/gXdEWVkZR48efePf58+fj+rq6ncYERE3bW1t/Otf/+pyuzt37jDXO+k5wqQjoggICMDr169x/vz5Ht1vT3NxccHw4cPx888/SzqUt5KWOEeOHAknJydcuXIF+fn5kg7njaQlThUVFUybNg0ZGRm4c+eOpMN5I2mJE6C0ibydpaUlJk6cCCMjI5SXlyM1NRW//voruFyupEPjIy3XMumdVq5cCRcXly63W7duHXJzc99BRF2jvKbnSUucXeG1ufC0tLSgrKwMOTk5+P3335GZmSnB6MiHhBqJpcS0adOgoKCAvLw8pKamwtDQEI6OjjAxMcHq1atRW1sr6RCFUl9fz2SMXWXu2dnZAICPPvpI7HF1R1NTE5M56evrw9zcHGlpaUzlvbGxUZLhETGoq6tjFUj69OmDcePGobCwEPfv32fe//PPPyUR3ju3b9++d3o8YdIRUXh4eKC2trbXFzotLCzg7u7e6xtipCVOAwMD+Pj4ICMjo1c3vkpLnMrKyvDx8QGAXl0hlpY4AUqbxOFd51/i4uXlhQULFqC8vBxJSUlQV1eHj48PLC0tsWHDBtTX10s6RBZpuZZJ73T//n28evWKeW1mZgYDAwPEx8ez6sbtt5E0ymt6nrTEKaj09HT8+eefkJOTg6amJkxNTWFmZob169ejqKhI0uGRDwA1EkuJ8PBw3Lt3j/Wen58fPvnkE0yaNImmyPUCjY2NTEXI29sb5ubmuHfv3nuTYRF+1dXVrMqvtrY2xo0bh4KCAqmoFBNCCCHk/eHv74/q6mps3LgR5eXlANoGmkyfPh3u7u6Ijo6WcISE9JzExEQkJiYyr1euXAkDAwNcuHCh14wcJkRYaWlprNnbvBHGDg4OfLO6CREHoRuJNTU1MX36dHA4HKirq6O4uBhZWVm4dOkSM+KTx87ODh9//DF0dHQgKyuLZ8+e4aeffuLbzsDAAJ9++ikMDQ1RUlKCy5cvY+jQoRgzZgwCAwOZEZjff/89qqqqsGnTJuazvOneZ8+exX/+8x/WfqdOnQpXV1doamqirq4OWVlZ+OGHH1i9icuXL4erqytCQ0Mxc+ZMjB49Go2Njbh48SKioqL4zl9dXR1BQUEYOXIkBgwYgJKSEjx69AiRkZGsZR8MDAwwb948DBo0CCoqKnjx4gWioqKQkJAg7FcOAHwNxABw9+5dfPLJJxg8eLBI+xQXFRUVzJs3DxwOBwMGDEBlZSVu3ryJ//73v2hpaRH78b29veHt7Q0tLS3U19cjNzcXP/zwA16+fCn2Y4uDl5cX7OzsYGBgAFVVVVRUVODJkyc4cuQIs3yFh4cHli1bhvj4eNjZ2aGiogKHDh1CUFAQ9PT0cOXKFfzyyy9CH9vAwABeXl4wMTGBjo4OmpqaUFJSgrNnz+LatWvdOq++ffsiODgY1tbWaGlpwf379xEWFoampiZmGwsLC3h4eGDo0KHQ1NRETU0NCgsL8csvvyAnJ4e1P2NjYyxYsAC6urooKCjAhQsXuhWfuPn4+MDV1RX6+vrIycnBwYMH8eLFC9Y2gqYj8vLyWLBgASwsLJh7LjU1FeHh4awR7GvWrIGVlRWOHDmCKVOmQFNTE+Xl5bhw4cIbOzO6ilNdXR1hYWHM66amJsyaNeuN5y1IGqqiogJfX1+MHj0aenp6UFRURFlZGW7fvi32dYd9fHwQFBTEei8iIoL5/4EDBxAfH8/6u4KCAoKCgmBpaYkBAwbg9u3bCAsL45s9IGieKAgDAwPs3bv3jXHGxcXh4MGDnZ5fT11371OcQNvySerq6szrdevWMf8vKChAaGgo32csLCzg5+eHkSNHorS0FL/88gtSUlJY2/R0niiuOAHByk2CWrNmDRwcHJjXPj4+zOgpAFi8eDEqKipYn9HV1RWoLCbpOCVxz1Pa1PP3vKD518CBA/Hvf/8bZ8+eRXl5eZdxCkqYMo4gbGxsoKmpiYSEBKaBGABiYmIwbdo0eHh4MI3EvEaHr7/+Gnfv3mW2XblyJZycnDBjxgyh4+Tt85tvvoGxsfEbrzthruX23337ep6XlxcWLVrExC+ucjAAmJiYwN/fH8OGDYOSkhIKCgqQlZWFY8eOsUZm9+3bF0uWLAGHw0FzczNu3LiB58+f49NPP8XevXtx8+ZNZltTU1P87W9/g56eHmRkZFBYWIj//ve/ePDgAd/xN2zYABMTE2zcuBEBAQEwNjaGpqYmCgsLsXXrVlRUVAhcbgoODoa5uTlyc3PB4XCQmZmJ8+fPY9GiRejTpw9+/vlnXL16lfU7/fjjj/Dw8IC+vj5qa2tx//59HD58mFVeB9rqIuPHj4eBgQEaGxvx/PlzpKSk9MpGLWnJayQRp7TkNeKMs6fzmu7ilRUVFBSY93htWO3TaqDtvCsqKljtZYDgbVgdaWtrY/v27WhqasL27dvFvswp6R2EbiResWIFhg8fjuvXryM9PR2ampowNzeHvr4+6wb39vbGggULkJOTw4xynTBhArZs2YLPPvuMKVD1798fW7duhaKiIiIjI1FTUwMfHx8oKyt368RCQ0Ph7OyMpKQkREZGQldXlzn+6tWrWdvKyMhgw4YNSEpKQlpaGlxdXTF79mzk5OQgLS2N2U5dXR179uxBv3798Mcff+Dp06f4y1/+AgsLC5iYmDA3zbBhw7B161ZwuVxcunQJJSUlcHR0xPLly9Ha2srq8ewOVVVVAHhj4+fy5cvh7u6OhIQEgdZN7YqOjg5sbW1Z78nJyaG5uZl5raSkhF27dkFTUxOxsbF48uQJjIyM4OPjAyUlJVZhXBx4o6sfPXqEqKgoaGtrY9KkSfj73/+O0NDQXjfNThA+Pj549OgR7t+/j5qaGlhZWcHJyQl9+vTB119/zdpWV1cXv//+Oz7++GNs3rwZV65cQUFBAXx9fXH58mUUFhYKdWwbGxsMHjwYd+/eRX5+PrS0tODl5YXly5ejrKwMjx49Evm8FixYgIcPH+Lo0aPgcDgYM2YMFBQU8N133zHbeHl5oaWlBfHx8SgpKcGwYcMwfvx4rF+/HqtWrUJdXR0AQE9PD5s2bUJdXR2T3syaNQvy8r1zsoS1tTX69++P+/fvo7GxESYmJli6dCm2bNnCbCNMOrJ69WrY2toiPj4e6enp4HA48PT0hJqaGnbv3s06try8PIKCgvDHH3+gqKgIzs7OCAoKApfLxZUrV4SOs6amhrUusJGR0RvPW9A0VFdXFy4uLkhOTkZCQgJkZGTg7u4Of39/1NXVibXCkZycjJKSEgDAwoULUV9fz6pgdTYyZebMmZCRkUFmZiaMjIzg4eGB8vJy1gwPQfNEQbVfj9nb2xtmZmasda9LS0v5PtPT1937FCfQVrBWVFTEqFGj4Ovri4iICOb37mwtTw0NDaxevRrp6el48OABLC0tsWrVKixdupRJm8SRJ4ojTkC4cpMgeM8TUFFRwbJly3Dr1i3W71JTU8P3mS+++AIVFRW4d+8eLC0tOy2L9YY4JXHPU9rU8/e8MPkXALi6uqKoqAiXL1+Gnp4exo8fzxenMAQt4whq+PDhANo6i9qrqKhAVVUVDA0N30mcQUFBSE9PR2RkJDgcDt91J8q1LKieLgerq6tj7dq1aG1txfXr1/H06VMMGjQIDg4OOH36NKtesXbtWowePRrR0dF4+vQpXFxc4OTkxLdPIyMjbNq0CeXl5Thx4gSamprg5+eHjRs3YuvWrXj8+HGnsXz22WcoKirC6dOn0adPH1hbW6Nfv36oqKgQqtykoaGBlJQUJCUlwd3dHSNGjEBERARcXFwQGBjINBLzzJkzB9euXcOFCxdgZGQET09PyMnJseqWVlZWWLZsGfLz83H27FmUlZVh+PDhsLGx6ZWNxNKS10giTmnJa8QV57tqxxFE3759YW5uDn9/f3C5XFZHkzAErX91pKmpiS1btqCpqQlbtmzpNP8m7yehWlA0NDRgbGyMP/74A0eOHGHej4iIQP/+/ZnXampqmDVrFrKysrB582amEfHq1av4/vvvMXXqVCZj8fHxgaqqKsLCwnD58mUAwIMHD/Dtt9+KfFLW1tZwdnbGuXPnWL3GKSkp2L59O8aNG4eYmBjmfRkZGZw/fx6XLl0CANy8eRPh4eFwdnZmJcKffPIJBgwYgH/+859ISkpi3j979iy0tLSY1/PmzUNjYyM+//xz5maKj4/Hl19+iWnTpvVY4jJ58mQ0Nze/s16tsWPHYuzYsXzvt08wpk2bBj09PezevRvJyckAgISEBFRUVODjjz/GuXPnhC6gCUpRURH+/v4oLS3Fjh07mF7BiooKLFq0CH5+fvjtt9/Ecmxx2rRpE+uBdwkJCWhubmZGsjx//pz525kzZ3Dnzh3Y2tpCV1cX4eHh0NTUhJubG0xMTIT+7mNiYvgKd0lJSfjmm28wceLEbjUSFxcXM6ONEhISoKamBicnJ5w8eZKp4Pz73/9mnfu1a9dQWFiIxYsXw8PDgxkt7O/vjz59+mD37t3MKIy8vDxs3boVVVVVIscoLhoaGli5ciUzYiAkJARubm5QV1dn3hM0HTE2NoatrS2SkpJw4MABAG3fp6KiIpydnWFiYsL6nWRlZXH8+HFmNBFvVNfkyZP5GokFibOxsVHgdYEFTUMLCwuxatUq1iiV2NhY7N27FxMmTBBrhaO4uJgpLM2ZM4e17vGbVFdXY9u2bUy8//rXv2BnZ8cUOoXJEwXVPi57e3sAXa9t15PX3fsWJwCkpqYC+F8HbG5u7ltj1dTUZI3CGz9+PBYvXgw7Oztm9Io48kRxxClsuUkQvIEDAwcOBACUlZW9NU5ZWVncuXOHeQCsjo4OvvvuO7i4uDBlsd4QJyCZe57Spp6/+7zTfQAAF8tJREFU54XJv3jbb9u2jRnVJScnBx8fH6ipqYk0Yk/QMo6geB3jnTUu19XVQU1NTegYRYnz8ePHzHUTHx8PAwMD1nUnyrUsqJ4uB1tYWEBVVRX79u3DjRs3mPdPnjzJGiQzevRojB49GomJiQgPDwfQdh91ttb1tGnTICcnhz179iAvLw9AW9133759mDZtGnbs2MH3mb59+yIlJYW1v/YzwIQpN7W0tODIkSOQk5ODq6srsrOzce7cObx+/RqLFy/GwIEDWXW7O3fuMKP/ExISoKysDBcXF0RERDC/o6OjI1paWrBlyxbmWomLi0O/fv0E+JbfLWnJayQVp7TkNeKIE3h37ThvExgYiMDAQOZ1UVERNm/eLPLzJwStf7WnoaGBLVu2oLW1lRqIP0CywmxcX1+P5uZmGBgY8CX67QsPRkZG6Nu3L65evcrKQGtra5GZmYlRo0Yx7w0dOpSvobOoqAhPnz4V+mR4Ro8eDQBMoy/Po0eP8OrVK5iamvJ9pn2iUldXh6qqKvzlL39hbWNsbIzCwkLWzQW0Zba83i95eXkYGRnhwYMHfDdTamoqdHR0oKOjI/K58YwZMwY2NjaIiorCkydPOt2muLgY2dnZPTYtIDY2Fnv27GH969gAx+FwUFpaylSGeZKTkyEnJ9fpd99TDA0NoaqqiuTkZNa0kfj4eDQ2NmLkyJFiO7Y4tb+3eNLT0wG0jZhoj3fN1dbWMhUrXsWlfUdOd45dUFCA4uJiaGpqCr2/9m7dusV6nZycDFlZWZiYmLz1+LwHwrXP1IYOHYqKigrWNL2MjIxem6Hdu3ePNaWM92A73jkJk47wvq/r16+ztuONBOFwOHzH73h/ZmRkQFdXl+8a6SpOYQmShgJtaXDHaYxNTU3IzMyEhoYGZGWFyrrE7tq1a6x4nz17Bg0NDea1MHmiOPXkdUdxts3iaT9Nm1cBbF92kGSeKEycopSbxIE3WAAAXrx4gerqatbSGr0lTmm556UlTmm55zMyMlhLxPBm8nUsiwlK0DKOoGRkZAB0PqPg9evXANoatoUlbJwdl7IpKSlhXXfi1NPlYF6D+8iRI1nfHZfLZdU1RowYAQC4ffs2815zczMr7eUZOnQocnJymAZioK2R98mTJ8x+OnPmzJm3xilouam6uhqNjY3gcrloaGhglibhjWTt2JnA65jkefDgAeTk5Fgj7+vq6iArK8uXBnc2OrY3kJa8RlrilJa8pqs4e0tew2tz2bdvH86ePYv+/ftjzZo1Ih9b0PoXT9++fbFlyxYoKipi69atvbY+TcRHqJHE9fX1+O233zB9+nSEh4ejoKAAqampSEpKYk2N4RUYFixYgAULFvDtp30Ba8CAAaiurkZDQwNrm5cvX741o3wbXuPV999/3+nfOzb+trS08K0B1NTUBCUlJea1vLw8Bg4c2GUPlb6+PgDAycmp0ylGQNvUeFHXLwPaMoRFixbhwYMHbx0ZGxERwVqbp7tevHjBd/6LFi1ivdbQ0IC6uvobj6utrd1j8XTE65ntuPxGQ0MDKisr31kBtaeZmZnB29sbgwcPhoaGBquQ2qdPH9a2vPuooaGB9f+Wlha+bQXRt29fTJ48GRYWFtDW1oaKigrzt2fPnolyOoyOvxOvM6P97/TRRx/Bz88PRkZG0NbWZq3FpKioyPyft7ZSR6WlpdDT0+tWnOLQMb3hVeh4nW/CpCO876tjmsIbjd0xvWttbWWtVQiA6ezR0dFhVQa7ilMYgqahPOPHj4eLiwt0dXUxYMAApvILtD0Zuv1TqyWt4/f5+vVrVv4hTJ4oTj153YmTtMRZWVnJes1rTOjbty/zniTzRB5B4hS23CQOra2tfJ3ar1+/Zi0/1hviBKTnnpeWOKXlnu/YWMprJGxfNhKGoGUcQfEa5Dpbsk9JSQmNjY2sBhpxxdnxe2pubmZdd+LU0+XgpKQk3LlzBz4+Phg/fjzy8vKQkpKCxMREVnrFa7jrmIZ1LJsqKCigX79+nS4pUVpaCiMjI6ioqPCVcVpaWliNyp0RtNzUvnG74/cEsPMGAHwDgnh5Svv09ty5czAzM8Pq1atRU1ODzMxM3Lt3D1evXu11S/1JS14jLXEC0pPXdBVnb8lr2re53LhxA9evX8euXbvwt7/9Df/85z+F2pew9S+gbbZ6U1MTFBUVoa2tLbXPdSKiE3rBztOnT+PWrVvw8PCAjo4OPD094ePjg4MHDzLTFltbWwEAJ06cYEYDvAlvW1F1tuZoc3MzWlpa8M0333S6f1F7NQVJwHiFrxs3brzxwV7dWWtr9OjRWLduHR4/foyvv/76nSWqgmptbUV+fj5r2kZ74k5UeTG8L4YPH44NGzagqqoK9+7dQ2ZmJurq6mBkZITJkycLNaKyfWFRUEuWLIGjoyPS0tKQnJyMP//8E62trZg3b55I+xOGsrIyPv/8c6ipqSE5ORnR0dEoLy9Hnz59sHLlSrEfX5y6um9FSUe6c93zvsuO++jp9EXQ/U2cOBHz589HXl4erl+/jidPnqCxsRHjxo2DlZVVrxtJ3NV5CZMnipM4rjtxeF/iBHpHniho2UUc5SZhtLa2CvTbSzpO4P2556Ulzt5yz/ckcZRxeCNmOzby8d4TpHO1Y/7aW8tiopQDRI11z549MDU1hbW1NQYPHoyPP/4Yfn5++OKLL1hLvvWUztI2Lpf71vtEnOWmjt9bZ9/jy5cvsXbtWri6umLYsGEwNjbGokWLMGHCBKxfv55vlLMkSUteIy1xApTXiFtOTo7AAyg7u9cFrX/xfqfi4mJ89dVX2LlzJ5YuXYq1a9f2us4eIl4iPdWpsLAQx44dA9A2Cmbv3r0YO3Ys00jMG8FWW1vbZa9FRUUFDA0NoaSkxJoe1dlUdi6Xy+q9Bjqf5lRcXAxZWVkUFBTwPbxBVE1NTSgrK8NHH3301u0KCwuZBKan1tfiMTAwQGhoKAoKCvCPf/yDb/R1b1BSUgItLS2hzr2hoUGgwgtvqtybtuX1tna8dhQVFTFgwABkZGR0+jljY2P0798fhYWFPXa99BQXFxcoKChgy5YtrN7koUOHiv3YioqKsLe3R2JiIvbv38+8Ly8vj5CQkE6nMwqj4+/EG1HH6600NTWFtrY2a71y4H9Tq9orKyvrtLe8u0tiSIow6QhvCpCOjg5rlAkvreo4RUhGRgYfffQRa10r3ih8ca0XDgiehgKAm5sbXrx4gfXr17MKNhMnTnzjZwRNR4TRUx1OwuSJouipOMWZfwEfXpyAaHmioHoyTnGUm8RBWuKUlnteWuIU9z0vCcKUcQSVlZUFAHz5rJqaGvr37896PgGvwt9xJHDHWXfiiLO9rq4RXiNXxzh55ZZ3JT09nVnqzc3NjVk3m1cfLisrA9BWlm1fFutYT21sbER1dXWnsxsHDhyIuro6oR9YyItJ2HKToDqeA6+83nGUdHNzM+Lj45n2gE8//RTjxo0Dh8PhW7Kit5OWvEbccVJe07u0bwerr6+HrKwsqw1NXl4eAwYMYNIjQLj6F68DKDY2FsXFxTh8+DDWrl2L+fPnM8+9IR8GoWrUKioqfCN3S0tLUV9fz5q68vDhQ1RUVMDb27vTKVjt1zpKT09nFs7n0dHRwbBhw/g+x3t6a/secg8PD77tbt68iZaWFkyfPr3T82i/lo8wbt++DT09vU6nH/Biam5uRnJyMuzs7DB48GC+7UR9aIS2tjY+//xzVFdXY9euXQL15kybNg1fffUVpk2bJtIxRZGUlAQNDY1OCyVKSkqdToF7+fIl5OXlu1wWoLS0FFwu943Tc//880+8fPkSNjY2rETUw8MDCgoKfGuk8cyYMQPr1q2Ds7PzW48vCXJycmhtbWU1yKqrq8PT0/OdHBtgT0sDgICAgE5/R2E5ODiwXtvY2KCpqYn5nXhpTfvOEHl5eUyePJlvXw8fPoSGhgar0sLhcN46zcrMzAwbNmzA4sWLu3Ue4iBMOnLv3j20tLTwpUuurq5obW3tdD289umtsrIyLCws8PTpU5EqJsIQJA0F2jqCmpqaWBUdMzOzt1ZKBU1HhFFTU9MjD10RJk8UBW9qb3c7RcSVf/FIS5y80Xjd3Q8gWp4oqJ6MU1zlJqCt7NbS0tIj95K0xCnue57SJraeuAfeNWHKOIJ6/vw58vLyYG5uzvo9x44dCxkZGVZ5gNeBPGTIEOY9Q0NDvpFq4oizva6uZS6Xi5qaGhgaGjLvKSkpwc7OrkeO3xXeA0Lb440ebj86ltcI2j4uOTk5WFlZ8X0+PT0dw4YNY52Tnp4eRowYwXpgujBEKTcJqmN53dbWFg0NDazngHT2PfFmyrxpUNPixYuxYcMGmJmZdTvGniYteY044wSkJ6/pqThFzWt8fHywYcMG+Pj4dDuGzhgaGkJTU5M1YIy3TFP7tcHHjRvX6Sx7QetfHSUlJSExMREeHh6wtbXtzikQKSPUSOJBgwYhJCQESUlJyMzMxMCBA+Hk5AQ1NTXmIUlAW2Zw/PhxLF26FLt27UJ8fDwKCgqgpaUFKysrcLlc7NmzBwAQExMDf39/fPLJJ+jXrx9qa2vh4+MDLpfLd9EmJSXBysoKX3zxBXJycmBkZNTpWlg5OTlISEjAmDFj0L9/f9y+fRtlZWUYPnw4bG1tER8fj7Nnzwr9ZZ0+fRrOzs5YsWIFRo4ciSdPnkBeXh7Ozs64ceMG03N64sQJmJubY/v27YiOjkZeXh769OkDCwsLjBgxAsHBwUIfe+PGjdDU1ER8fDwCAgJYf6uoqOj0YQba2toYPnz4O+0BvXjxIsaOHYt58+ZhyJAhyMjIQGNjIzgcDmxtbfHVV1/xTdNITExEQEAA1qxZg6ioKHC5XOTl5XW6/s2tW7fg5uaGWbNmITs7G69fv2YKVM3Nzfj9998xf/58fPnll4iNjYW2tjYmTZqEsrIyvkX9eXijD7s7FUpBQQGzZ88G8L81jaysrJjK0a+//ir06Nu7d+/C29sbmzdvRmxsLDQ1NTFmzBiUlJR0O+PvSn19PTIzM+Hu7o6Wlhbk5ubC1dUVhoaGPbKAvb6+PlavXo27d++Cw+HAysoKCQkJTKb38OFDVFdXY86cOdDT00NtbS0mTJjQaUNmdHQ0xo8fj5CQEERFRUFWVhb+/v5vbfQcOHAgrK2txTp6tjsETUfy8/Nx69YtODk5YdmyZUhPTweHw4GzszNu377Nt4Zdc3MzvL29mZEH48aNg7KyMi5evChSnCYmJkzBTE1NDTIyMqyCRGpqKlNBEDQNTU1NxZQpU7BhwwbcuXMHZmZmcHBwQF5eXqeFNkC4dERQWVlZ8PX1RWhoKJKTk/H69Wvk5uYKff0LkyeK4uHDh/Dz80NoaCji4uLw6tUrlJaWijQlThz5l7TFmZ2dDS6Xi6lTp0JVVRX5+fngcrmsCrGgRMkTJRGnuMpNQFua8/TpUzg6OqK6uhqPHz9GS0sLUlJS+Doh35c4xX3PU9rUM/e8MPlXTxOmjCOMK1euYMGCBVi/fj1ycnIAAF5eXigoKMCFCxeY7bKyslBUVARfX18oKyujX79+sLa2RlFREauzVVxxto+jq2s5OTkZHh4eWLVqFerq6mBhYdHt2WyC8vb2hqWlJZKTk5Gfn49hw4bB1dUVXC4XN27cYLbLzc1Famoq3NzcUFtbi5ycHLi4uGDAgAF8+zxz5gzs7e2xfv16nD9/Hk1NTfDz80Nra6vI6Zco5SZBDRo0CKtWrcK9e/dgbm4OU1NTxMTEsNYqDgkJQVNTE+7fv4+amhqMGjUKbm5ueP78+RsfsM7hcKCnp8d62F9vIS15jTjjBKQnr+mpOAHR8prBgwfD2tq604d8isLMzIxpkJaVlYWzszNkZGQQGRnJbHP9+nXMmDEDwcHBuHnzJvT19TFkyBC+NcQBwetfnfnpp58wevRoLFmyBNnZ2XzPECDvJ6EaiSsqKlBUVIRRo0bB09MTra2teP78OcLCwvgurri4OFRWVmLGjBnw9fWFqqoqysvLkZaWxmpQ5nK52LZtG5YsWYKAgAAoKiri1q1baGho4BslHBsbi0GDBsHFxQVaWlpISUlBdHQ0du/ezRfrwYMHkZ+fj7FjxyIwMBDKysp49uwZ0tLSOh1ZJ4iqqiqsW7cOQUFBsLe3h7e3NxoaGpCUlMSawpWfn4/169dj7ty5cHd3x+TJk9HQ0ID09HScPn1apGPzRkR2NnI6Pz//rU+8fZeam5uxceNGzJkzB2ZmZnBxcYGcnByysrIQExPDtwA/0NbTvH//fkyfPh0hISGQkZHBoUOHcOXKFb5tjx49ChUVFfz1r3+FkpISioqKsGLFCubvFy9eRGtrK7y9vbF48WLU19fj8ePHOHTo0BszXn19fTQ1NSExMbFb5y4vL8/Xg2hmZsb0kJ86dUroQm1KSgrCwsIwYcIEzJo1C4WFhThz5gyKi4sRGhrarXgFceDAAQQGBsLOzg729vZ49OgRtm7diuDgYJGekN3e8ePHMXHiRDg6OqKyshJxcXEICwtj/v7q1Svs27cPAQEBmDhxIqqqqpCamorIyEgcPHiQta/i4mLs2LEDCxcuxOzZsyEnJ4crV65AS0vrnSzNIQ7CpCP79+9HbW0tLC0t4eLigsrKSsTExODHH3/k229zczN+/vlnzJ07F0pKSnj58iXCw8ORkJAgUpyzZ89m9WIDwLp165j/L126lCmkCZqGnjx5EnJycrC3tweHw0F2dja+/fZbmJqavrGyI0w6IqhTp05BRUUFpqamcHR0hKysLA4cOPDWwtSbCJoniiI5ORnHjh2Dm5sbFi5cCDk5OcTFxfHdJ4IQR/4lbXG+evUK+/fvx+TJkzFlyhQoKSmhoKBApDRXlDxREnEC4ik38YSFhWHmzJlwdXWFr68vgLZRZKJUNqQlTnHe85Q29cw9L0z+1dOEKeMIIzo6GoqKivDy8sKIESMgLy+PmpoabN68mW8wxHfffYdFixZh7NixyM3NxeHDh2FlZcVqJBZXnDyCXMtHjx5Fv379YGVlhZcvXyI+Ph4VFRV8D88Wh5ycHIwePRpOTk7Q1dVFTU0NcnJycOHCBb71iPfu3Yvg4GA4Oztj4sSJyMnJQVRUFAIDA/n2uW3bNsydOxczZsyAjIwMCgsL8cMPP7yxQbUropSbBHX8+HH4+vrCyckJFRUViImJwZEjR1jbPHz4ELa2tvD19YWmpibKy8uRlJSEyMjIXrUesTCkJa8RZ5zSktf0ZJziLF8KytTUFKampmhpaUFFRQVycnJw8uRJZGZmMtsUFxfj4MGDmDp1Kjw9PZGRkYGdO3dizZo1fPsTtP7VmZqaGhw6dAiff/45QkJCsH379h4/X9L7yDg4OPTKp3wFBwdjzJgxCAwMFLpXjRBBjRw5Ejt37sSlS5dw+PBhSYdDiFitWbMGVlZWzIh3QgghhLy/pkyZglmzZiElJQW7d+9m1tsk74aHhweWLVuGvXv34ubNm5IORyg+Pj4ICgrC9u3bRZqhQgghRDqJ9OA6Qt4XZmZmaGhowG+//SbpUAghhBBCCOkxvOnJDg4O0NbW7rVLbBFCCCGkd6BGYvJBO3nyJE6ePCnpMAghhBBCCOlxkZGRrLUsCSGEEELeRFbSARBCCCGEEEIIIYQQQgiRnF67JjEhhBBCCCGEEEIIIYQQ8aORxIQQQgghhBBCCCGEEPIBo0ZiQgghhBBCCCGEEEII+YBRIzEhhBBCCCGEEEIIIYR8wKiRmBBCCCGEEEIIIYQQQj5g1EhMCCGEEEIIIYQQQgghHzBqJCaEEEIIIYQQQgghhJAPGDUSE0IIIYQQQgghhBBCyAeMGokJIYQQQgghhBBCCCHkA0aNxIQQQgghhBBCCCGEEPIBo0ZiQgghhBBCCCGEEEII+YBRIzEhhBBCCCGEEEIIIYR8wKiRmBBCCCGEEEIIIYQQQj5g1EhMCCGEEEIIIYQQQgghHzD5hoYGScdACCGEEEIIIYQQQgghRELkZWVpMDEhhBBCCCGEEEIIIYR8qKiRmBBCCCGEEEIIIYQQQj5g8jIyMpKOgRBCCCGEEEIIIYQQQoiEUCMxIYQQQgghhBBCCCGEfMBorQlCCCGEEEIIIYQQQgj5gP0/LWTZSrEdL90AAAAASUVORK5CYII=)"
      ],
      "metadata": {
        "id": "57ziKwJaPozK"
      }
    }
  ]
}